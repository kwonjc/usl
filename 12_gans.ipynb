{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 불러오기\n",
    "'''메인 라이브러리'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time, re\n",
    "import pickle, gzip, datetime\n",
    "\n",
    "'''시각화 관련 라이브러리'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1 import Grid\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "'''데이터 준비 및 모델 평가 관련 라이브러리'''\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, mean_squared_error\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "'''알고리즘 관련 라이브러리'''\n",
    "import lightgbm as lgb\n",
    "\n",
    "'''텐서플로 및 케라스 관련 라이브러리'''\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.layers import LeakyReLU, Reshape, UpSampling2D, Conv2DTranspose\n",
    "from keras.layers import BatchNormalization, Input, Lambda\n",
    "from keras.layers import Embedding, Flatten, dot\n",
    "from keras import regularizers\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "current_path = os.getcwd()\n",
    "file = os.path.sep.join(['', 'datasets', 'mnist_data', 'mnist.pkl.gz'])\n",
    "f = gzip.open(current_path+file, 'rb')\n",
    "train_set, validation_set, test_set = pickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    "\n",
    "X_train, y_train = train_set[0], train_set[1]\n",
    "X_validation, y_validation = validation_set[0], validation_set[1]\n",
    "X_test, y_test = test_set[0], test_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_keras = X_train.reshape(50000,28,28,1)\n",
    "X_validation_keras = X_validation.reshape(10000,28,28,1)\n",
    "X_test_keras = X_test.reshape(10000,28,28,1)\n",
    "\n",
    "y_train_keras = to_categorical(y_train)\n",
    "y_validation_keras = to_categorical(y_validation)\n",
    "y_test_keras = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋으로부터 판다스 데이터 프레임 만들기\n",
    "train_index = range(0,len(X_train))\n",
    "validation_index = range(len(X_train),len(X_train)+len(X_validation))\n",
    "test_index = range(len(X_train)+len(X_validation),len(X_train)+ \\\n",
    "                   len(X_validation)+len(X_test))\n",
    "\n",
    "X_train = pd.DataFrame(data=X_train,index=train_index)\n",
    "y_train = pd.Series(data=y_train,index=train_index)\n",
    "\n",
    "X_validation = pd.DataFrame(data=X_validation,index=validation_index)\n",
    "y_validation = pd.Series(data=y_validation,index=validation_index)\n",
    "\n",
    "X_test = pd.DataFrame(data=X_test,index=test_index)\n",
    "y_test = pd.Series(data=y_test,index=test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_digit(X, y, example):\n",
    "    label = y.loc[example]\n",
    "    image = X.loc[example,:].values.reshape([28,28])\n",
    "    plt.title('Example: %d  Label: %d' % (example, label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 첫번째 숫자 이미지 보기\n",
    "view_digit(X_train, y_train, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPU version of TF\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용 확인\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else: print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컨볼루션 신경망(CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Surface Pro6\\.conda\\envs\\unsupervisedLearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Surface Pro6\\.conda\\envs\\unsupervisedLearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Surface Pro6\\.conda\\envs\\unsupervisedLearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Surface Pro6\\.conda\\envs\\unsupervisedLearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Surface Pro6\\.conda\\envs\\unsupervisedLearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Surface Pro6\\.conda\\envs\\unsupervisedLearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', \n",
    "                 activation ='relu', input_shape = (28,28,1)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Surface Pro6\\.conda\\envs\\unsupervisedLearning\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Surface Pro6\\.conda\\envs\\unsupervisedLearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Surface Pro6\\.conda\\envs\\unsupervisedLearning\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 164s 3ms/step - loss: 0.1894 - acc: 0.9404 - val_loss: 0.0533 - val_acc: 0.9850\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 166s 3ms/step - loss: 0.0733 - acc: 0.9777 - val_loss: 0.0338 - val_acc: 0.9888\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 166s 3ms/step - loss: 0.0529 - acc: 0.9848 - val_loss: 0.0328 - val_acc: 0.9903\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 177s 4ms/step - loss: 0.0461 - acc: 0.9861 - val_loss: 0.0327 - val_acc: 0.9923\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 174s 3ms/step - loss: 0.0395 - acc: 0.9877 - val_loss: 0.0359 - val_acc: 0.9906\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 183s 4ms/step - loss: 0.0352 - acc: 0.9888 - val_loss: 0.0334 - val_acc: 0.9926\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 0.0339 - acc: 0.9898 - val_loss: 0.0294 - val_acc: 0.9926\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 178s 4ms/step - loss: 0.0297 - acc: 0.9906 - val_loss: 0.0322 - val_acc: 0.9923\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 188s 4ms/step - loss: 0.0278 - acc: 0.9919 - val_loss: 0.0261 - val_acc: 0.9936\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 181s 4ms/step - loss: 0.0274 - acc: 0.9919 - val_loss: 0.0265 - val_acc: 0.9935\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 179s 4ms/step - loss: 0.0246 - acc: 0.9925 - val_loss: 0.0299 - val_acc: 0.9928\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 0.0240 - acc: 0.9927 - val_loss: 0.0288 - val_acc: 0.9941\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 0.0238 - acc: 0.9924 - val_loss: 0.0309 - val_acc: 0.9926\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 184s 4ms/step - loss: 0.0237 - acc: 0.9928 - val_loss: 0.0313 - val_acc: 0.9932\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 181s 4ms/step - loss: 0.0221 - acc: 0.9935 - val_loss: 0.0406 - val_acc: 0.9913\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 179s 4ms/step - loss: 0.0226 - acc: 0.9934 - val_loss: 0.0311 - val_acc: 0.9923\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 181s 4ms/step - loss: 0.0223 - acc: 0.9934 - val_loss: 0.0343 - val_acc: 0.9924\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 181s 4ms/step - loss: 0.0202 - acc: 0.9941 - val_loss: 0.0284 - val_acc: 0.9938\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 187s 4ms/step - loss: 0.0215 - acc: 0.9937 - val_loss: 0.0382 - val_acc: 0.9932\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 183s 4ms/step - loss: 0.0195 - acc: 0.9943 - val_loss: 0.0335 - val_acc: 0.9939\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 0.0197 - acc: 0.9940 - val_loss: 0.0269 - val_acc: 0.9935\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 0.0220 - acc: 0.9937 - val_loss: 0.0353 - val_acc: 0.9926\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 183s 4ms/step - loss: 0.0206 - acc: 0.9942 - val_loss: 0.0318 - val_acc: 0.9930\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 0.0190 - acc: 0.9944 - val_loss: 0.0313 - val_acc: 0.9938\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 0.0206 - acc: 0.9941 - val_loss: 0.0287 - val_acc: 0.9939\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 181s 4ms/step - loss: 0.0184 - acc: 0.9947 - val_loss: 0.0290 - val_acc: 0.9936\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 0.0174 - acc: 0.9951 - val_loss: 0.0447 - val_acc: 0.9931\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 178s 4ms/step - loss: 0.0202 - acc: 0.9945 - val_loss: 0.0483 - val_acc: 0.9905\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 181s 4ms/step - loss: 0.0198 - acc: 0.9945 - val_loss: 0.0400 - val_acc: 0.9935\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 180s 4ms/step - loss: 0.0164 - acc: 0.9950 - val_loss: 0.0417 - val_acc: 0.9932\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 0.0194 - acc: 0.9944 - val_loss: 0.0322 - val_acc: 0.9945\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 181s 4ms/step - loss: 0.0189 - acc: 0.9940 - val_loss: 0.0441 - val_acc: 0.9918\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 181s 4ms/step - loss: 0.0189 - acc: 0.9952 - val_loss: 0.0398 - val_acc: 0.9938\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 0.0197 - acc: 0.9945 - val_loss: 0.0311 - val_acc: 0.9939\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 0.0195 - acc: 0.9943 - val_loss: 0.0375 - val_acc: 0.9930\n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 179s 4ms/step - loss: 0.0187 - acc: 0.9950 - val_loss: 0.0346 - val_acc: 0.9936\n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 181s 4ms/step - loss: 0.0184 - acc: 0.9951 - val_loss: 0.0404 - val_acc: 0.9928\n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 181s 4ms/step - loss: 0.0191 - acc: 0.9947 - val_loss: 0.0356 - val_acc: 0.9940\n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 185s 4ms/step - loss: 0.0190 - acc: 0.9946 - val_loss: 0.0309 - val_acc: 0.9941\n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 187s 4ms/step - loss: 0.0188 - acc: 0.9954 - val_loss: 0.0434 - val_acc: 0.9939\n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 191s 4ms/step - loss: 0.0201 - acc: 0.9950 - val_loss: 0.0344 - val_acc: 0.9942\n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 184s 4ms/step - loss: 0.0202 - acc: 0.9948 - val_loss: 0.0332 - val_acc: 0.9928\n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 180s 4ms/step - loss: 0.0180 - acc: 0.9950 - val_loss: 0.0298 - val_acc: 0.9927\n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 183s 4ms/step - loss: 0.0203 - acc: 0.9945 - val_loss: 0.0487 - val_acc: 0.9917\n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 183s 4ms/step - loss: 0.0166 - acc: 0.9957 - val_loss: 0.0409 - val_acc: 0.9928\n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 183s 4ms/step - loss: 0.0214 - acc: 0.9946 - val_loss: 0.0397 - val_acc: 0.9939\n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 183s 4ms/step - loss: 0.0178 - acc: 0.9953 - val_loss: 0.0318 - val_acc: 0.9948\n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 183s 4ms/step - loss: 0.0208 - acc: 0.9947 - val_loss: 0.0334 - val_acc: 0.9941\n",
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 180s 4ms/step - loss: 0.0205 - acc: 0.9949 - val_loss: 0.0470 - val_acc: 0.9926\n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 186s 4ms/step - loss: 0.0207 - acc: 0.9949 - val_loss: 0.0354 - val_acc: 0.9939\n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 183s 4ms/step - loss: 0.0219 - acc: 0.9945 - val_loss: 0.0382 - val_acc: 0.9945\n",
      "Epoch 52/100\n",
      "50000/50000 [==============================] - 180s 4ms/step - loss: 0.0182 - acc: 0.9952 - val_loss: 0.0358 - val_acc: 0.9942\n",
      "Epoch 53/100\n",
      "50000/50000 [==============================] - 183s 4ms/step - loss: 0.0250 - acc: 0.9942 - val_loss: 0.0316 - val_acc: 0.9935\n",
      "Epoch 54/100\n",
      "50000/50000 [==============================] - 179s 4ms/step - loss: 0.0191 - acc: 0.9953 - val_loss: 0.0357 - val_acc: 0.9939\n",
      "Epoch 55/100\n",
      "50000/50000 [==============================] - 179s 4ms/step - loss: 0.0234 - acc: 0.9944 - val_loss: 0.0400 - val_acc: 0.9928\n",
      "Epoch 56/100\n",
      "50000/50000 [==============================] - 180s 4ms/step - loss: 0.0207 - acc: 0.9943 - val_loss: 0.0390 - val_acc: 0.9935\n",
      "Epoch 57/100\n",
      "50000/50000 [==============================] - 177s 4ms/step - loss: 0.0218 - acc: 0.9949 - val_loss: 0.0482 - val_acc: 0.9933\n",
      "Epoch 58/100\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 0.0249 - acc: 0.9947 - val_loss: 0.0399 - val_acc: 0.9942\n",
      "Epoch 59/100\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 0.0217 - acc: 0.9949 - val_loss: 0.0383 - val_acc: 0.9939\n",
      "Epoch 60/100\n",
      "50000/50000 [==============================] - 181s 4ms/step - loss: 0.0250 - acc: 0.9945 - val_loss: 0.0378 - val_acc: 0.9938\n",
      "Epoch 61/100\n",
      "50000/50000 [==============================] - 180s 4ms/step - loss: 0.0198 - acc: 0.9954 - val_loss: 0.0394 - val_acc: 0.9940\n",
      "Epoch 62/100\n",
      "50000/50000 [==============================] - 180s 4ms/step - loss: 0.0236 - acc: 0.9947 - val_loss: 0.0434 - val_acc: 0.9935\n",
      "Epoch 63/100\n",
      "50000/50000 [==============================] - 177s 4ms/step - loss: 0.0250 - acc: 0.9948 - val_loss: 0.0360 - val_acc: 0.9933\n",
      "Epoch 64/100\n",
      "50000/50000 [==============================] - 180s 4ms/step - loss: 0.0234 - acc: 0.9945 - val_loss: 0.0419 - val_acc: 0.9926\n",
      "Epoch 65/100\n",
      "50000/50000 [==============================] - 180s 4ms/step - loss: 0.0184 - acc: 0.9956 - val_loss: 0.0406 - val_acc: 0.9936\n",
      "Epoch 66/100\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 0.0236 - acc: 0.9945 - val_loss: 0.0352 - val_acc: 0.9942\n",
      "Epoch 67/100\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 0.0190 - acc: 0.9954 - val_loss: 0.0546 - val_acc: 0.9925\n",
      "Epoch 68/100\n",
      "50000/50000 [==============================] - 180s 4ms/step - loss: 0.0227 - acc: 0.9946 - val_loss: 0.0480 - val_acc: 0.9939\n",
      "Epoch 69/100\n",
      "50000/50000 [==============================] - 178s 4ms/step - loss: 0.0194 - acc: 0.9954 - val_loss: 0.0355 - val_acc: 0.9948\n",
      "Epoch 70/100\n",
      "50000/50000 [==============================] - 184s 4ms/step - loss: 0.0232 - acc: 0.9949 - val_loss: 0.0440 - val_acc: 0.9934\n",
      "Epoch 71/100\n",
      "50000/50000 [==============================] - 189s 4ms/step - loss: 0.0212 - acc: 0.9950 - val_loss: 0.0393 - val_acc: 0.9937\n",
      "Epoch 72/100\n",
      "50000/50000 [==============================] - 181s 4ms/step - loss: 0.0200 - acc: 0.9948 - val_loss: 0.0449 - val_acc: 0.9937\n",
      "Epoch 73/100\n",
      "50000/50000 [==============================] - 183s 4ms/step - loss: 0.0217 - acc: 0.9948 - val_loss: 0.0444 - val_acc: 0.9935\n",
      "Epoch 74/100\n",
      "50000/50000 [==============================] - 181s 4ms/step - loss: 0.0222 - acc: 0.9951 - val_loss: 0.0537 - val_acc: 0.9930\n",
      "Epoch 75/100\n",
      "50000/50000 [==============================] - 177s 4ms/step - loss: 0.0215 - acc: 0.9953 - val_loss: 0.0456 - val_acc: 0.9933\n",
      "Epoch 76/100\n",
      "50000/50000 [==============================] - 181s 4ms/step - loss: 0.0228 - acc: 0.9952 - val_loss: 0.0546 - val_acc: 0.9935\n",
      "Epoch 77/100\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 0.0204 - acc: 0.9959 - val_loss: 0.0398 - val_acc: 0.9940\n",
      "Epoch 78/100\n",
      "50000/50000 [==============================] - 181s 4ms/step - loss: 0.0251 - acc: 0.9949 - val_loss: 0.0439 - val_acc: 0.9945\n",
      "Epoch 79/100\n",
      "50000/50000 [==============================] - 183s 4ms/step - loss: 0.0241 - acc: 0.9948 - val_loss: 0.0376 - val_acc: 0.9945\n",
      "Epoch 80/100\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 0.0230 - acc: 0.9948 - val_loss: 0.0437 - val_acc: 0.9938\n",
      "Epoch 81/100\n",
      "50000/50000 [==============================] - 178s 4ms/step - loss: 0.0187 - acc: 0.9960 - val_loss: 0.0583 - val_acc: 0.9939\n",
      "Epoch 82/100\n",
      "50000/50000 [==============================] - 181s 4ms/step - loss: 0.0231 - acc: 0.9952 - val_loss: 0.0403 - val_acc: 0.9949\n",
      "Epoch 83/100\n",
      "50000/50000 [==============================] - 178s 4ms/step - loss: 0.0227 - acc: 0.9950 - val_loss: 0.0398 - val_acc: 0.9940\n",
      "Epoch 84/100\n",
      "50000/50000 [==============================] - 181s 4ms/step - loss: 0.0262 - acc: 0.9949 - val_loss: 0.0540 - val_acc: 0.9935\n",
      "Epoch 85/100\n",
      "50000/50000 [==============================] - 181s 4ms/step - loss: 0.0247 - acc: 0.9952 - val_loss: 0.0450 - val_acc: 0.9934\n",
      "Epoch 86/100\n",
      "50000/50000 [==============================] - 179s 4ms/step - loss: 0.0283 - acc: 0.9948 - val_loss: 0.0449 - val_acc: 0.9941\n",
      "Epoch 87/100\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 0.0289 - acc: 0.9944 - val_loss: 0.0435 - val_acc: 0.9936\n",
      "Epoch 88/100\n",
      "50000/50000 [==============================] - 178s 4ms/step - loss: 0.0228 - acc: 0.9953 - val_loss: 0.0423 - val_acc: 0.9932\n",
      "Epoch 89/100\n",
      "50000/50000 [==============================] - 181s 4ms/step - loss: 0.0219 - acc: 0.9955 - val_loss: 0.0472 - val_acc: 0.9936\n",
      "Epoch 90/100\n",
      "50000/50000 [==============================] - 181s 4ms/step - loss: 0.0245 - acc: 0.9951 - val_loss: 0.0483 - val_acc: 0.9928\n",
      "Epoch 91/100\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 0.0242 - acc: 0.9951 - val_loss: 0.0432 - val_acc: 0.9934\n",
      "Epoch 92/100\n",
      "50000/50000 [==============================] - 178s 4ms/step - loss: 0.0299 - acc: 0.9944 - val_loss: 0.0444 - val_acc: 0.9936\n",
      "Epoch 93/100\n",
      "50000/50000 [==============================] - 183s 4ms/step - loss: 0.0272 - acc: 0.9943 - val_loss: 0.0515 - val_acc: 0.9932\n",
      "Epoch 94/100\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 0.0238 - acc: 0.9949 - val_loss: 0.0364 - val_acc: 0.9949\n",
      "Epoch 95/100\n",
      "50000/50000 [==============================] - 178s 4ms/step - loss: 0.0210 - acc: 0.9956 - val_loss: 0.0436 - val_acc: 0.9934\n",
      "Epoch 96/100\n",
      "50000/50000 [==============================] - 181s 4ms/step - loss: 0.0241 - acc: 0.9950 - val_loss: 0.0453 - val_acc: 0.9937\n",
      "Epoch 97/100\n",
      "50000/50000 [==============================] - 180s 4ms/step - loss: 0.0284 - acc: 0.9947 - val_loss: 0.0514 - val_acc: 0.9948\n",
      "Epoch 98/100\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 0.0254 - acc: 0.9952 - val_loss: 0.0473 - val_acc: 0.9947\n",
      "Epoch 99/100\n",
      "50000/50000 [==============================] - 179s 4ms/step - loss: 0.0256 - acc: 0.9948 - val_loss: 0.0432 - val_acc: 0.9937\n",
      "Epoch 100/100\n",
      "50000/50000 [==============================] - 182s 4ms/step - loss: 0.0264 - acc: 0.9947 - val_loss: 0.0468 - val_acc: 0.9943\n"
     ]
    }
   ],
   "source": [
    "# CNN 훈련\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cnn_history = model.fit(X_train_keras, y_train_keras, \n",
    "          validation_data=(X_validation_keras, y_validation_keras), \\\n",
    "          epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    }
   ],
   "source": [
    "print(cnn_history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Final Accuracy 0.99466\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## CNN 정확도 그래프\n",
    "print(\"CNN Final Accuracy\", cnn_history.history['acc'][-1])\n",
    "pd.Series(cnn_history.history['acc']).plot(logy=False)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "    def elapsed(self,sec):\n",
    "        if sec < 60:\n",
    "            return str(sec) + \" sec\"\n",
    "        elif sec < (60 * 60):\n",
    "            return str(sec / 60) + \" min\"\n",
    "        else:\n",
    "            return str(sec / (60 * 60)) + \" hr\"\n",
    "    def elapsed_time(self):\n",
    "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time))\n",
    "\n",
    "class DCGAN(object):\n",
    "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
    "\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channel = channel\n",
    "        self.D = None   # 감별자\n",
    "        self.G = None   # 생성자\n",
    "        self.AM = None  # 적대 모델\n",
    "        self.DM = None  # 판별 모델\n",
    "        \n",
    "    def generator(self, depth=256, dim=7, dropout=0.3, momentum=0.8, \\\n",
    "                  window=5, input_dim=100, output_depth=1):\n",
    "        if self.G:\n",
    "            return self.G\n",
    "        self.G = Sequential()\n",
    "        self.G.add(Dense(dim*dim*depth, input_dim=input_dim))\n",
    "        self.G.add(BatchNormalization(momentum=momentum))\n",
    "        self.G.add(Activation('relu'))\n",
    "        self.G.add(Reshape((dim, dim, depth)))\n",
    "        self.G.add(Dropout(dropout))\n",
    "        \n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/2), window, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=momentum))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/4), window, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=momentum))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(Conv2DTranspose(int(depth/8), window, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=momentum))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(Conv2DTranspose(output_depth, window, padding='same'))\n",
    "        self.G.add(Activation('sigmoid'))\n",
    "        self.G.summary()\n",
    "        return self.G\n",
    "\n",
    "    def discriminator(self, depth=64, dropout=0.3, alpha=0.3):\n",
    "        if self.D:\n",
    "            return self.D\n",
    "        self.D = Sequential()\n",
    "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
    "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\n",
    "            padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=alpha))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=alpha))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=alpha))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=alpha))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Flatten())\n",
    "        self.D.add(Dense(1))\n",
    "        self.D.add(Activation('sigmoid'))\n",
    "        self.D.summary()\n",
    "        return self.D\n",
    "\n",
    "    def discriminator_model(self):\n",
    "        if self.DM:\n",
    "            return self.DM\n",
    "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "        self.DM = Sequential()\n",
    "        self.DM.add(self.discriminator())\n",
    "        self.DM.compile(loss='binary_crossentropy', \\\n",
    "                        optimizer=optimizer, metrics=['accuracy'])\n",
    "        return self.DM\n",
    "\n",
    "    def adversarial_model(self):\n",
    "        if self.AM:\n",
    "            return self.AM\n",
    "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "        self.AM = Sequential()\n",
    "        self.AM.add(self.generator())\n",
    "        self.AM.add(self.discriminator())\n",
    "        self.AM.compile(loss='binary_crossentropy', \\\n",
    "                        optimizer=optimizer, metrics=['accuracy'])\n",
    "        return self.AM\n",
    "        \n",
    "class MNIST_DCGAN(object):\n",
    "    def __init__(self, x_train):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channel = 1\n",
    "\n",
    "        self.x_train = x_train\n",
    "\n",
    "        self.DCGAN = DCGAN()\n",
    "        self.discriminator =  self.DCGAN.discriminator_model()\n",
    "        self.adversarial = self.DCGAN.adversarial_model()\n",
    "        self.generator = self.DCGAN.generator()\n",
    "\n",
    "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "        for i in range(train_steps):\n",
    "            images_train = self.x_train[np.random.randint(0,\n",
    "                self.x_train.shape[0], size=batch_size), :, :, :]\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            images_fake = self.generator.predict(noise)\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            \n",
    "            d_loss = self.discriminator.train_on_batch(x, y)\n",
    "\n",
    "            y = np.ones([batch_size, 1])\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], \\\n",
    "                                                      a_loss[1])\n",
    "            print(log_mesg)\n",
    "            if save_interval>0:\n",
    "                if (i+1)%save_interval==0:\n",
    "                    self.plot_images(save2file=True, \\\n",
    "                        samples=noise_input.shape[0],\\\n",
    "                        noise=noise_input, step=(i+1))\n",
    "\n",
    "    def plot_images(self, save2file=False, fake=True, samples=16, \\\n",
    "                    noise=None, step=0):\n",
    "        current_path = os.getcwd()\n",
    "        file = os.path.sep.join(['', 'images', 'chapter12', 'synthetic_mnist', ''])\n",
    "        filename = 'mnist.png'\n",
    "        if fake:\n",
    "            if noise is None:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "            else:\n",
    "                filename = \"mnist_%d.png\" % step\n",
    "            images = self.generator.predict(noise)\n",
    "        else:\n",
    "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
    "            images = self.x_train[i, :, :, :]\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(images.shape[0]):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            image = images[i, :, :, :]\n",
    "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        if save2file:\n",
    "            plt.savefig(current_path+file+filename)\n",
    "            plt.close('all')\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,311,553\n",
      "Trainable params: 4,311,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Surface Pro6\\.conda\\envs\\unsupervisedLearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Surface Pro6\\.conda\\envs\\unsupervisedLearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 12544)             1266944   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 2,394,241\n",
      "Trainable params: 2,368,705\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n",
      "0: [D loss: 0.691304, acc: 0.603516]  [A loss: 2.288928, acc: 0.000000]\n",
      "1: [D loss: 0.648292, acc: 0.951172]  [A loss: 0.673236, acc: 0.707031]\n",
      "2: [D loss: 0.996560, acc: 0.500000]  [A loss: 2.909399, acc: 0.000000]\n",
      "3: [D loss: 0.444713, acc: 0.984375]  [A loss: 2.696861, acc: 0.000000]\n",
      "4: [D loss: 0.424303, acc: 0.945312]  [A loss: 6.781157, acc: 0.000000]\n",
      "5: [D loss: 0.512456, acc: 0.820312]  [A loss: 0.845331, acc: 0.203125]\n",
      "6: [D loss: 1.223485, acc: 0.498047]  [A loss: 9.810223, acc: 0.000000]\n",
      "7: [D loss: 0.748930, acc: 0.548828]  [A loss: 0.852105, acc: 0.242188]\n",
      "8: [D loss: 1.067603, acc: 0.488281]  [A loss: 4.526834, acc: 0.000000]\n",
      "9: [D loss: 0.478353, acc: 0.841797]  [A loss: 1.409293, acc: 0.003906]\n",
      "10: [D loss: 0.925244, acc: 0.476562]  [A loss: 6.446825, acc: 0.000000]\n",
      "11: [D loss: 0.618830, acc: 0.667969]  [A loss: 0.745883, acc: 0.421875]\n",
      "12: [D loss: 1.125642, acc: 0.494141]  [A loss: 5.857840, acc: 0.000000]\n",
      "13: [D loss: 0.599928, acc: 0.710938]  [A loss: 0.640396, acc: 0.660156]\n",
      "14: [D loss: 1.193792, acc: 0.496094]  [A loss: 6.219239, acc: 0.000000]\n",
      "15: [D loss: 0.652663, acc: 0.677734]  [A loss: 0.571058, acc: 0.765625]\n",
      "16: [D loss: 1.180578, acc: 0.490234]  [A loss: 4.928374, acc: 0.000000]\n",
      "17: [D loss: 0.564360, acc: 0.777344]  [A loss: 0.916435, acc: 0.179688]\n",
      "18: [D loss: 1.069486, acc: 0.490234]  [A loss: 5.573259, acc: 0.000000]\n",
      "19: [D loss: 0.608084, acc: 0.701172]  [A loss: 0.549275, acc: 0.820312]\n",
      "20: [D loss: 1.096685, acc: 0.494141]  [A loss: 4.656874, acc: 0.000000]\n",
      "21: [D loss: 0.568032, acc: 0.740234]  [A loss: 0.751228, acc: 0.390625]\n",
      "22: [D loss: 1.037522, acc: 0.500000]  [A loss: 5.642721, acc: 0.000000]\n",
      "23: [D loss: 0.635252, acc: 0.646484]  [A loss: 0.415206, acc: 0.949219]\n",
      "24: [D loss: 1.149031, acc: 0.496094]  [A loss: 4.700707, acc: 0.000000]\n",
      "25: [D loss: 0.519315, acc: 0.792969]  [A loss: 0.734833, acc: 0.453125]\n",
      "26: [D loss: 1.037235, acc: 0.498047]  [A loss: 5.382295, acc: 0.000000]\n",
      "27: [D loss: 0.567091, acc: 0.736328]  [A loss: 0.490259, acc: 0.906250]\n",
      "28: [D loss: 1.080459, acc: 0.500000]  [A loss: 4.697205, acc: 0.000000]\n",
      "29: [D loss: 0.498725, acc: 0.800781]  [A loss: 0.690058, acc: 0.558594]\n",
      "30: [D loss: 1.007600, acc: 0.500000]  [A loss: 5.009033, acc: 0.000000]\n",
      "31: [D loss: 0.528900, acc: 0.753906]  [A loss: 0.540835, acc: 0.832031]\n",
      "32: [D loss: 1.045926, acc: 0.500000]  [A loss: 4.774778, acc: 0.000000]\n",
      "33: [D loss: 0.449451, acc: 0.828125]  [A loss: 0.690957, acc: 0.519531]\n",
      "34: [D loss: 1.008800, acc: 0.500000]  [A loss: 5.458563, acc: 0.000000]\n",
      "35: [D loss: 0.509146, acc: 0.789062]  [A loss: 0.444337, acc: 0.933594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36: [D loss: 1.130640, acc: 0.500000]  [A loss: 5.120434, acc: 0.000000]\n",
      "37: [D loss: 0.466640, acc: 0.816406]  [A loss: 0.593685, acc: 0.750000]\n",
      "38: [D loss: 1.012480, acc: 0.498047]  [A loss: 5.111442, acc: 0.000000]\n",
      "39: [D loss: 0.413751, acc: 0.873047]  [A loss: 0.716864, acc: 0.464844]\n",
      "40: [D loss: 1.005464, acc: 0.500000]  [A loss: 6.015405, acc: 0.000000]\n",
      "41: [D loss: 0.460717, acc: 0.785156]  [A loss: 0.345922, acc: 0.992188]\n",
      "42: [D loss: 1.169728, acc: 0.500000]  [A loss: 5.284122, acc: 0.000000]\n",
      "43: [D loss: 0.404098, acc: 0.859375]  [A loss: 0.592992, acc: 0.746094]\n",
      "44: [D loss: 1.076239, acc: 0.500000]  [A loss: 5.559307, acc: 0.000000]\n",
      "45: [D loss: 0.398750, acc: 0.878906]  [A loss: 0.505917, acc: 0.851562]\n",
      "46: [D loss: 1.091951, acc: 0.500000]  [A loss: 5.667561, acc: 0.000000]\n",
      "47: [D loss: 0.381177, acc: 0.859375]  [A loss: 0.559675, acc: 0.769531]\n",
      "48: [D loss: 1.086157, acc: 0.500000]  [A loss: 5.680429, acc: 0.000000]\n",
      "49: [D loss: 0.383475, acc: 0.876953]  [A loss: 0.486300, acc: 0.855469]\n",
      "50: [D loss: 1.140573, acc: 0.500000]  [A loss: 5.783136, acc: 0.000000]\n",
      "51: [D loss: 0.356355, acc: 0.904297]  [A loss: 0.607658, acc: 0.671875]\n",
      "52: [D loss: 1.158141, acc: 0.500000]  [A loss: 6.428534, acc: 0.000000]\n",
      "53: [D loss: 0.394133, acc: 0.833984]  [A loss: 0.330088, acc: 0.968750]\n",
      "54: [D loss: 1.209535, acc: 0.500000]  [A loss: 5.562110, acc: 0.000000]\n",
      "55: [D loss: 0.300320, acc: 0.955078]  [A loss: 0.789408, acc: 0.394531]\n",
      "56: [D loss: 1.096755, acc: 0.500000]  [A loss: 6.812568, acc: 0.000000]\n",
      "57: [D loss: 0.487797, acc: 0.763672]  [A loss: 0.114025, acc: 1.000000]\n",
      "58: [D loss: 1.380418, acc: 0.500000]  [A loss: 3.700586, acc: 0.000000]\n",
      "59: [D loss: 0.377262, acc: 0.873047]  [A loss: 2.120048, acc: 0.000000]\n",
      "60: [D loss: 0.734732, acc: 0.513672]  [A loss: 5.752954, acc: 0.000000]\n",
      "61: [D loss: 0.459174, acc: 0.820312]  [A loss: 0.193851, acc: 1.000000]\n",
      "62: [D loss: 1.208253, acc: 0.500000]  [A loss: 4.227327, acc: 0.000000]\n",
      "63: [D loss: 0.386689, acc: 0.925781]  [A loss: 0.597313, acc: 0.695312]\n",
      "64: [D loss: 1.010407, acc: 0.500000]  [A loss: 5.253811, acc: 0.000000]\n",
      "65: [D loss: 0.506671, acc: 0.791016]  [A loss: 0.130795, acc: 1.000000]\n",
      "66: [D loss: 1.339674, acc: 0.500000]  [A loss: 2.744054, acc: 0.000000]\n",
      "67: [D loss: 0.463675, acc: 0.718750]  [A loss: 2.203900, acc: 0.000000]\n",
      "68: [D loss: 0.657402, acc: 0.519531]  [A loss: 4.646732, acc: 0.000000]\n",
      "69: [D loss: 0.454971, acc: 0.853516]  [A loss: 0.284072, acc: 0.992188]\n",
      "70: [D loss: 1.141746, acc: 0.500000]  [A loss: 4.731725, acc: 0.000000]\n",
      "71: [D loss: 0.540465, acc: 0.787109]  [A loss: 0.108016, acc: 1.000000]\n",
      "72: [D loss: 1.407178, acc: 0.500000]  [A loss: 1.963284, acc: 0.000000]\n",
      "73: [D loss: 0.527347, acc: 0.609375]  [A loss: 2.744139, acc: 0.000000]\n",
      "74: [D loss: 0.451043, acc: 0.730469]  [A loss: 2.171734, acc: 0.000000]\n",
      "75: [D loss: 0.594603, acc: 0.537109]  [A loss: 4.321968, acc: 0.000000]\n",
      "76: [D loss: 0.421706, acc: 0.886719]  [A loss: 0.321782, acc: 0.988281]\n",
      "77: [D loss: 1.142710, acc: 0.500000]  [A loss: 4.836895, acc: 0.000000]\n",
      "78: [D loss: 0.666143, acc: 0.691406]  [A loss: 0.041436, acc: 1.000000]\n",
      "79: [D loss: 1.712067, acc: 0.500000]  [A loss: 0.759459, acc: 0.382812]\n",
      "80: [D loss: 0.789232, acc: 0.500000]  [A loss: 2.853083, acc: 0.000000]\n",
      "81: [D loss: 0.422540, acc: 0.857422]  [A loss: 1.101436, acc: 0.074219]\n",
      "82: [D loss: 0.780639, acc: 0.500000]  [A loss: 4.248957, acc: 0.000000]\n",
      "83: [D loss: 0.505297, acc: 0.830078]  [A loss: 0.148898, acc: 1.000000]\n",
      "84: [D loss: 1.295798, acc: 0.500000]  [A loss: 2.210597, acc: 0.000000]\n",
      "85: [D loss: 0.506862, acc: 0.648438]  [A loss: 1.962707, acc: 0.000000]\n",
      "86: [D loss: 0.600265, acc: 0.531250]  [A loss: 3.428086, acc: 0.000000]\n",
      "87: [D loss: 0.440567, acc: 0.841797]  [A loss: 0.731836, acc: 0.453125]\n",
      "88: [D loss: 1.019280, acc: 0.500000]  [A loss: 5.593052, acc: 0.000000]\n",
      "89: [D loss: 0.788789, acc: 0.619141]  [A loss: 0.029497, acc: 1.000000]\n",
      "90: [D loss: 1.842723, acc: 0.500000]  [A loss: 0.391123, acc: 0.972656]\n",
      "91: [D loss: 0.968018, acc: 0.500000]  [A loss: 2.033379, acc: 0.000000]\n",
      "92: [D loss: 0.488674, acc: 0.656250]  [A loss: 1.885615, acc: 0.000000]\n",
      "93: [D loss: 0.606238, acc: 0.537109]  [A loss: 2.686149, acc: 0.000000]\n",
      "94: [D loss: 0.530106, acc: 0.660156]  [A loss: 2.221192, acc: 0.000000]\n",
      "95: [D loss: 0.635749, acc: 0.537109]  [A loss: 3.678719, acc: 0.000000]\n",
      "96: [D loss: 0.479841, acc: 0.839844]  [A loss: 0.318173, acc: 0.984375]\n",
      "97: [D loss: 1.158187, acc: 0.500000]  [A loss: 4.815005, acc: 0.000000]\n",
      "98: [D loss: 0.716183, acc: 0.650391]  [A loss: 0.028123, acc: 1.000000]\n",
      "99: [D loss: 1.866556, acc: 0.500000]  [A loss: 0.526174, acc: 0.832031]\n",
      "100: [D loss: 0.936197, acc: 0.500000]  [A loss: 2.610054, acc: 0.000000]\n",
      "101: [D loss: 0.495001, acc: 0.748047]  [A loss: 0.945943, acc: 0.171875]\n",
      "102: [D loss: 0.859427, acc: 0.505859]  [A loss: 4.066860, acc: 0.000000]\n",
      "103: [D loss: 0.608984, acc: 0.771484]  [A loss: 0.107083, acc: 1.000000]\n",
      "104: [D loss: 1.443220, acc: 0.500000]  [A loss: 2.019211, acc: 0.000000]\n",
      "105: [D loss: 0.572557, acc: 0.593750]  [A loss: 2.016619, acc: 0.000000]\n",
      "106: [D loss: 0.639419, acc: 0.539062]  [A loss: 3.248970, acc: 0.000000]\n",
      "107: [D loss: 0.520082, acc: 0.732422]  [A loss: 1.056191, acc: 0.117188]\n",
      "108: [D loss: 1.009294, acc: 0.500000]  [A loss: 6.013216, acc: 0.000000]\n",
      "109: [D loss: 1.032780, acc: 0.529297]  [A loss: 0.013944, acc: 1.000000]\n",
      "110: [D loss: 2.070094, acc: 0.500000]  [A loss: 0.246009, acc: 1.000000]\n",
      "111: [D loss: 1.050044, acc: 0.500000]  [A loss: 1.365394, acc: 0.011719]\n",
      "112: [D loss: 0.620393, acc: 0.546875]  [A loss: 1.911470, acc: 0.000000]\n",
      "113: [D loss: 0.582631, acc: 0.587891]  [A loss: 1.714308, acc: 0.003906]\n",
      "114: [D loss: 0.658300, acc: 0.515625]  [A loss: 2.653329, acc: 0.000000]\n",
      "115: [D loss: 0.550840, acc: 0.642578]  [A loss: 1.490708, acc: 0.000000]\n",
      "116: [D loss: 0.785025, acc: 0.501953]  [A loss: 4.276461, acc: 0.000000]\n",
      "117: [D loss: 0.627729, acc: 0.714844]  [A loss: 0.058646, acc: 1.000000]\n",
      "118: [D loss: 1.659465, acc: 0.500000]  [A loss: 2.046907, acc: 0.000000]\n",
      "119: [D loss: 0.569894, acc: 0.605469]  [A loss: 1.590013, acc: 0.000000]\n",
      "120: [D loss: 0.728090, acc: 0.525391]  [A loss: 3.655626, acc: 0.000000]\n",
      "121: [D loss: 0.567493, acc: 0.757812]  [A loss: 0.236938, acc: 0.992188]\n",
      "122: [D loss: 1.238778, acc: 0.500000]  [A loss: 3.899858, acc: 0.000000]\n",
      "123: [D loss: 0.664638, acc: 0.677734]  [A loss: 0.065758, acc: 1.000000]\n",
      "124: [D loss: 1.615908, acc: 0.500000]  [A loss: 1.556273, acc: 0.000000]\n",
      "125: [D loss: 0.653640, acc: 0.537109]  [A loss: 2.461248, acc: 0.000000]\n",
      "126: [D loss: 0.587302, acc: 0.623047]  [A loss: 1.576049, acc: 0.000000]\n",
      "127: [D loss: 0.753257, acc: 0.515625]  [A loss: 4.330929, acc: 0.000000]\n",
      "128: [D loss: 0.643707, acc: 0.685547]  [A loss: 0.060429, acc: 1.000000]\n",
      "129: [D loss: 1.629839, acc: 0.500000]  [A loss: 2.306057, acc: 0.000000]\n",
      "130: [D loss: 0.580348, acc: 0.632812]  [A loss: 1.175835, acc: 0.062500]\n",
      "131: [D loss: 0.789561, acc: 0.509766]  [A loss: 4.169590, acc: 0.000000]\n",
      "132: [D loss: 0.613761, acc: 0.710938]  [A loss: 0.089210, acc: 1.000000]\n",
      "133: [D loss: 1.548360, acc: 0.500000]  [A loss: 2.736197, acc: 0.000000]\n",
      "134: [D loss: 0.575546, acc: 0.710938]  [A loss: 0.447964, acc: 0.910156]\n",
      "135: [D loss: 1.026009, acc: 0.500000]  [A loss: 4.064397, acc: 0.000000]\n",
      "136: [D loss: 0.635763, acc: 0.699219]  [A loss: 0.087230, acc: 1.000000]\n",
      "137: [D loss: 1.607414, acc: 0.500000]  [A loss: 2.354450, acc: 0.000000]\n",
      "138: [D loss: 0.595716, acc: 0.630859]  [A loss: 1.175817, acc: 0.105469]\n",
      "139: [D loss: 0.836379, acc: 0.503906]  [A loss: 4.123554, acc: 0.000000]\n",
      "140: [D loss: 0.621617, acc: 0.708984]  [A loss: 0.105265, acc: 1.000000]\n",
      "141: [D loss: 1.468737, acc: 0.500000]  [A loss: 3.246468, acc: 0.000000]\n",
      "142: [D loss: 0.568253, acc: 0.732422]  [A loss: 0.371579, acc: 0.968750]\n",
      "143: [D loss: 1.081623, acc: 0.500000]  [A loss: 3.929300, acc: 0.000000]\n",
      "144: [D loss: 0.659237, acc: 0.666016]  [A loss: 0.096417, acc: 1.000000]\n",
      "145: [D loss: 1.507921, acc: 0.500000]  [A loss: 2.428925, acc: 0.000000]\n",
      "146: [D loss: 0.585267, acc: 0.625000]  [A loss: 1.210248, acc: 0.074219]\n",
      "147: [D loss: 0.827839, acc: 0.503906]  [A loss: 4.065853, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148: [D loss: 0.629178, acc: 0.671875]  [A loss: 0.142134, acc: 1.000000]\n",
      "149: [D loss: 1.391781, acc: 0.500000]  [A loss: 3.813511, acc: 0.000000]\n",
      "150: [D loss: 0.623599, acc: 0.679688]  [A loss: 0.151918, acc: 1.000000]\n",
      "151: [D loss: 1.359521, acc: 0.500000]  [A loss: 3.179872, acc: 0.000000]\n",
      "152: [D loss: 0.550744, acc: 0.730469]  [A loss: 0.415308, acc: 0.890625]\n",
      "153: [D loss: 1.126185, acc: 0.500000]  [A loss: 4.330553, acc: 0.000000]\n",
      "154: [D loss: 0.657150, acc: 0.658203]  [A loss: 0.065557, acc: 1.000000]\n",
      "155: [D loss: 1.661976, acc: 0.500000]  [A loss: 2.225899, acc: 0.000000]\n",
      "156: [D loss: 0.604967, acc: 0.587891]  [A loss: 1.773468, acc: 0.003906]\n",
      "157: [D loss: 0.778967, acc: 0.507812]  [A loss: 4.000479, acc: 0.000000]\n",
      "158: [D loss: 0.583381, acc: 0.666016]  [A loss: 0.354892, acc: 0.949219]\n",
      "159: [D loss: 1.230137, acc: 0.500000]  [A loss: 5.825819, acc: 0.000000]\n",
      "160: [D loss: 0.953789, acc: 0.546875]  [A loss: 0.013041, acc: 1.000000]\n",
      "161: [D loss: 2.402236, acc: 0.500000]  [A loss: 0.713012, acc: 0.496094]\n",
      "162: [D loss: 0.869934, acc: 0.500000]  [A loss: 2.524833, acc: 0.000000]\n",
      "163: [D loss: 0.603611, acc: 0.617188]  [A loss: 1.108740, acc: 0.136719]\n",
      "164: [D loss: 0.921022, acc: 0.500000]  [A loss: 3.888122, acc: 0.000000]\n",
      "165: [D loss: 0.599759, acc: 0.658203]  [A loss: 0.470023, acc: 0.828125]\n",
      "166: [D loss: 1.134596, acc: 0.498047]  [A loss: 4.992970, acc: 0.000000]\n",
      "167: [D loss: 0.722571, acc: 0.636719]  [A loss: 0.059902, acc: 1.000000]\n",
      "168: [D loss: 1.673651, acc: 0.500000]  [A loss: 2.445715, acc: 0.000000]\n",
      "169: [D loss: 0.584734, acc: 0.621094]  [A loss: 1.724155, acc: 0.007812]\n",
      "170: [D loss: 0.824398, acc: 0.511719]  [A loss: 3.673502, acc: 0.000000]\n",
      "171: [D loss: 0.619646, acc: 0.628906]  [A loss: 0.510569, acc: 0.820312]\n",
      "172: [D loss: 1.135761, acc: 0.501953]  [A loss: 5.514421, acc: 0.000000]\n",
      "173: [D loss: 0.859020, acc: 0.580078]  [A loss: 0.020096, acc: 1.000000]\n",
      "174: [D loss: 2.205627, acc: 0.500000]  [A loss: 1.326490, acc: 0.019531]\n",
      "175: [D loss: 0.744280, acc: 0.533203]  [A loss: 2.238767, acc: 0.000000]\n",
      "176: [D loss: 0.651525, acc: 0.570312]  [A loss: 1.672383, acc: 0.007812]\n",
      "177: [D loss: 0.786735, acc: 0.519531]  [A loss: 3.583311, acc: 0.000000]\n",
      "178: [D loss: 0.616070, acc: 0.642578]  [A loss: 0.641595, acc: 0.621094]\n",
      "179: [D loss: 1.112070, acc: 0.500000]  [A loss: 5.506078, acc: 0.000000]\n",
      "180: [D loss: 0.797013, acc: 0.593750]  [A loss: 0.029068, acc: 1.000000]\n",
      "181: [D loss: 2.036882, acc: 0.500000]  [A loss: 2.084195, acc: 0.003906]\n",
      "182: [D loss: 0.648287, acc: 0.570312]  [A loss: 2.148446, acc: 0.000000]\n",
      "183: [D loss: 0.666483, acc: 0.572266]  [A loss: 2.498429, acc: 0.000000]\n",
      "184: [D loss: 0.694311, acc: 0.548828]  [A loss: 2.703532, acc: 0.000000]\n",
      "185: [D loss: 0.702962, acc: 0.550781]  [A loss: 2.273539, acc: 0.000000]\n",
      "186: [D loss: 0.855308, acc: 0.517578]  [A loss: 5.741220, acc: 0.000000]\n",
      "187: [D loss: 0.762416, acc: 0.623047]  [A loss: 0.012990, acc: 1.000000]\n",
      "188: [D loss: 2.458839, acc: 0.500000]  [A loss: 2.477302, acc: 0.000000]\n",
      "189: [D loss: 0.617577, acc: 0.615234]  [A loss: 1.614996, acc: 0.023438]\n",
      "190: [D loss: 0.890297, acc: 0.501953]  [A loss: 4.837022, acc: 0.000000]\n",
      "191: [D loss: 0.711258, acc: 0.623047]  [A loss: 0.049499, acc: 1.000000]\n",
      "192: [D loss: 1.859207, acc: 0.500000]  [A loss: 3.214495, acc: 0.000000]\n",
      "193: [D loss: 0.603350, acc: 0.656250]  [A loss: 0.553610, acc: 0.750000]\n",
      "194: [D loss: 1.094594, acc: 0.498047]  [A loss: 4.805978, acc: 0.000000]\n",
      "195: [D loss: 0.751539, acc: 0.583984]  [A loss: 0.060349, acc: 1.000000]\n",
      "196: [D loss: 1.819938, acc: 0.500000]  [A loss: 2.378336, acc: 0.000000]\n",
      "197: [D loss: 0.614908, acc: 0.638672]  [A loss: 1.143857, acc: 0.105469]\n",
      "198: [D loss: 0.908688, acc: 0.505859]  [A loss: 4.064316, acc: 0.000000]\n",
      "199: [D loss: 0.631912, acc: 0.677734]  [A loss: 0.141347, acc: 1.000000]\n",
      "200: [D loss: 1.439116, acc: 0.500000]  [A loss: 3.980001, acc: 0.000000]\n",
      "201: [D loss: 0.660545, acc: 0.662109]  [A loss: 0.127499, acc: 1.000000]\n",
      "202: [D loss: 1.404476, acc: 0.500000]  [A loss: 3.425461, acc: 0.000000]\n",
      "203: [D loss: 0.617040, acc: 0.654297]  [A loss: 0.304737, acc: 0.968750]\n",
      "204: [D loss: 1.224673, acc: 0.501953]  [A loss: 4.174684, acc: 0.000000]\n",
      "205: [D loss: 0.637339, acc: 0.679688]  [A loss: 0.187326, acc: 1.000000]\n",
      "206: [D loss: 1.337417, acc: 0.500000]  [A loss: 3.454207, acc: 0.000000]\n",
      "207: [D loss: 0.603087, acc: 0.693359]  [A loss: 0.326214, acc: 0.960938]\n",
      "208: [D loss: 1.171764, acc: 0.501953]  [A loss: 4.041548, acc: 0.000000]\n",
      "209: [D loss: 0.640601, acc: 0.656250]  [A loss: 0.226909, acc: 0.992188]\n",
      "210: [D loss: 1.323654, acc: 0.500000]  [A loss: 3.861536, acc: 0.000000]\n",
      "211: [D loss: 0.635823, acc: 0.656250]  [A loss: 0.274924, acc: 0.980469]\n",
      "212: [D loss: 1.302229, acc: 0.500000]  [A loss: 4.242083, acc: 0.000000]\n",
      "213: [D loss: 0.655316, acc: 0.646484]  [A loss: 0.154644, acc: 1.000000]\n",
      "214: [D loss: 1.426456, acc: 0.500000]  [A loss: 3.345708, acc: 0.000000]\n",
      "215: [D loss: 0.600955, acc: 0.667969]  [A loss: 0.473744, acc: 0.843750]\n",
      "216: [D loss: 1.193537, acc: 0.500000]  [A loss: 4.774220, acc: 0.000000]\n",
      "217: [D loss: 0.647820, acc: 0.666016]  [A loss: 0.143085, acc: 1.000000]\n",
      "218: [D loss: 1.537373, acc: 0.500000]  [A loss: 3.934580, acc: 0.000000]\n",
      "219: [D loss: 0.573348, acc: 0.716797]  [A loss: 0.283781, acc: 0.960938]\n",
      "220: [D loss: 1.258041, acc: 0.500000]  [A loss: 4.310240, acc: 0.000000]\n",
      "221: [D loss: 0.622209, acc: 0.671875]  [A loss: 0.229785, acc: 0.992188]\n",
      "222: [D loss: 1.403751, acc: 0.500000]  [A loss: 4.675815, acc: 0.000000]\n",
      "223: [D loss: 0.697946, acc: 0.628906]  [A loss: 0.113907, acc: 1.000000]\n",
      "224: [D loss: 1.636050, acc: 0.498047]  [A loss: 3.522987, acc: 0.000000]\n",
      "225: [D loss: 0.594799, acc: 0.650391]  [A loss: 0.681964, acc: 0.582031]\n",
      "226: [D loss: 1.211069, acc: 0.498047]  [A loss: 5.633506, acc: 0.000000]\n",
      "227: [D loss: 0.668699, acc: 0.650391]  [A loss: 0.091770, acc: 1.000000]\n",
      "228: [D loss: 1.762233, acc: 0.500000]  [A loss: 3.763343, acc: 0.000000]\n",
      "229: [D loss: 0.632800, acc: 0.656250]  [A loss: 0.780689, acc: 0.492188]\n",
      "230: [D loss: 1.192228, acc: 0.503906]  [A loss: 5.866365, acc: 0.000000]\n",
      "231: [D loss: 0.761709, acc: 0.623047]  [A loss: 0.028831, acc: 1.000000]\n",
      "232: [D loss: 2.234290, acc: 0.500000]  [A loss: 2.617199, acc: 0.000000]\n",
      "233: [D loss: 0.715994, acc: 0.560547]  [A loss: 2.528746, acc: 0.000000]\n",
      "234: [D loss: 0.829910, acc: 0.521484]  [A loss: 4.440264, acc: 0.000000]\n",
      "235: [D loss: 0.639062, acc: 0.611328]  [A loss: 0.664325, acc: 0.574219]\n",
      "236: [D loss: 1.276763, acc: 0.503906]  [A loss: 7.536452, acc: 0.000000]\n",
      "237: [D loss: 1.080077, acc: 0.535156]  [A loss: 0.005228, acc: 1.000000]\n",
      "238: [D loss: 3.097860, acc: 0.500000]  [A loss: 1.247083, acc: 0.125000]\n",
      "239: [D loss: 0.974978, acc: 0.501953]  [A loss: 3.910369, acc: 0.000000]\n",
      "240: [D loss: 0.643890, acc: 0.625000]  [A loss: 0.791414, acc: 0.457031]\n",
      "241: [D loss: 1.173389, acc: 0.500000]  [A loss: 5.278651, acc: 0.000000]\n",
      "242: [D loss: 0.718298, acc: 0.640625]  [A loss: 0.095817, acc: 0.996094]\n",
      "243: [D loss: 1.719585, acc: 0.501953]  [A loss: 3.752398, acc: 0.000000]\n",
      "244: [D loss: 0.637916, acc: 0.619141]  [A loss: 1.262607, acc: 0.152344]\n",
      "245: [D loss: 1.160545, acc: 0.501953]  [A loss: 5.938172, acc: 0.000000]\n",
      "246: [D loss: 0.844588, acc: 0.564453]  [A loss: 0.033720, acc: 1.000000]\n",
      "247: [D loss: 2.316479, acc: 0.500000]  [A loss: 3.233099, acc: 0.000000]\n",
      "248: [D loss: 0.659361, acc: 0.617188]  [A loss: 1.912245, acc: 0.007812]\n",
      "249: [D loss: 0.988879, acc: 0.498047]  [A loss: 5.400098, acc: 0.000000]\n",
      "250: [D loss: 0.746634, acc: 0.611328]  [A loss: 0.104444, acc: 0.996094]\n",
      "251: [D loss: 1.669429, acc: 0.500000]  [A loss: 5.031529, acc: 0.000000]\n",
      "252: [D loss: 0.707560, acc: 0.640625]  [A loss: 0.088950, acc: 0.996094]\n",
      "253: [D loss: 1.717134, acc: 0.500000]  [A loss: 3.847437, acc: 0.000000]\n",
      "254: [D loss: 0.613038, acc: 0.644531]  [A loss: 0.563395, acc: 0.714844]\n",
      "255: [D loss: 1.285188, acc: 0.498047]  [A loss: 5.756657, acc: 0.000000]\n",
      "256: [D loss: 0.845402, acc: 0.562500]  [A loss: 0.034115, acc: 1.000000]\n",
      "257: [D loss: 2.325369, acc: 0.500000]  [A loss: 2.960587, acc: 0.000000]\n",
      "258: [D loss: 0.719698, acc: 0.562500]  [A loss: 2.866137, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259: [D loss: 0.870947, acc: 0.519531]  [A loss: 4.403214, acc: 0.000000]\n",
      "260: [D loss: 0.677291, acc: 0.603516]  [A loss: 0.901927, acc: 0.414062]\n",
      "261: [D loss: 1.370507, acc: 0.496094]  [A loss: 8.237375, acc: 0.000000]\n",
      "262: [D loss: 1.299264, acc: 0.519531]  [A loss: 0.008255, acc: 1.000000]\n",
      "263: [D loss: 3.255571, acc: 0.500000]  [A loss: 1.268170, acc: 0.132812]\n",
      "264: [D loss: 1.058719, acc: 0.515625]  [A loss: 4.212512, acc: 0.000000]\n",
      "265: [D loss: 0.667143, acc: 0.609375]  [A loss: 0.593218, acc: 0.664062]\n",
      "266: [D loss: 1.255504, acc: 0.503906]  [A loss: 5.090504, acc: 0.000000]\n",
      "267: [D loss: 0.701051, acc: 0.607422]  [A loss: 0.203607, acc: 0.964844]\n",
      "268: [D loss: 1.487876, acc: 0.498047]  [A loss: 4.883289, acc: 0.000000]\n",
      "269: [D loss: 0.739297, acc: 0.587891]  [A loss: 0.131439, acc: 1.000000]\n",
      "270: [D loss: 1.491327, acc: 0.500000]  [A loss: 4.197680, acc: 0.000000]\n",
      "271: [D loss: 0.661066, acc: 0.630859]  [A loss: 0.508325, acc: 0.746094]\n",
      "272: [D loss: 1.227783, acc: 0.500000]  [A loss: 5.417428, acc: 0.000000]\n",
      "273: [D loss: 0.755434, acc: 0.617188]  [A loss: 0.065789, acc: 1.000000]\n",
      "274: [D loss: 1.870994, acc: 0.500000]  [A loss: 4.047317, acc: 0.000000]\n",
      "275: [D loss: 0.642324, acc: 0.630859]  [A loss: 0.750932, acc: 0.492188]\n",
      "276: [D loss: 1.333103, acc: 0.507812]  [A loss: 6.221634, acc: 0.000000]\n",
      "277: [D loss: 0.832852, acc: 0.603516]  [A loss: 0.038020, acc: 1.000000]\n",
      "278: [D loss: 2.304803, acc: 0.500000]  [A loss: 2.901198, acc: 0.000000]\n",
      "279: [D loss: 0.739352, acc: 0.583984]  [A loss: 2.641493, acc: 0.007812]\n",
      "280: [D loss: 0.871780, acc: 0.507812]  [A loss: 3.985012, acc: 0.000000]\n",
      "281: [D loss: 0.763658, acc: 0.544922]  [A loss: 2.512169, acc: 0.000000]\n",
      "282: [D loss: 1.218619, acc: 0.507812]  [A loss: 8.479601, acc: 0.000000]\n",
      "283: [D loss: 1.372602, acc: 0.523438]  [A loss: 0.002855, acc: 1.000000]\n",
      "284: [D loss: 3.872621, acc: 0.500000]  [A loss: 1.066789, acc: 0.257812]\n",
      "285: [D loss: 1.218387, acc: 0.507812]  [A loss: 5.024527, acc: 0.000000]\n",
      "286: [D loss: 0.706468, acc: 0.613281]  [A loss: 0.200959, acc: 0.968750]\n",
      "287: [D loss: 1.440801, acc: 0.505859]  [A loss: 3.973936, acc: 0.000000]\n",
      "288: [D loss: 0.676467, acc: 0.630859]  [A loss: 1.403256, acc: 0.125000]\n",
      "289: [D loss: 1.121536, acc: 0.511719]  [A loss: 5.451797, acc: 0.000000]\n",
      "290: [D loss: 0.746899, acc: 0.611328]  [A loss: 0.087251, acc: 1.000000]\n",
      "291: [D loss: 1.713326, acc: 0.500000]  [A loss: 4.735283, acc: 0.000000]\n",
      "292: [D loss: 0.666219, acc: 0.617188]  [A loss: 0.367794, acc: 0.859375]\n",
      "293: [D loss: 1.370770, acc: 0.503906]  [A loss: 5.842673, acc: 0.000000]\n",
      "294: [D loss: 0.836527, acc: 0.595703]  [A loss: 0.037423, acc: 1.000000]\n",
      "295: [D loss: 2.223883, acc: 0.500000]  [A loss: 2.995735, acc: 0.003906]\n",
      "296: [D loss: 0.730277, acc: 0.582031]  [A loss: 2.496386, acc: 0.003906]\n",
      "297: [D loss: 0.872310, acc: 0.523438]  [A loss: 4.215528, acc: 0.000000]\n",
      "298: [D loss: 0.696344, acc: 0.591797]  [A loss: 2.017292, acc: 0.023438]\n",
      "299: [D loss: 1.214143, acc: 0.517578]  [A loss: 7.743904, acc: 0.000000]\n",
      "300: [D loss: 1.204265, acc: 0.523438]  [A loss: 0.005323, acc: 1.000000]\n",
      "301: [D loss: 3.490755, acc: 0.500000]  [A loss: 1.351936, acc: 0.167969]\n",
      "302: [D loss: 1.124694, acc: 0.515625]  [A loss: 4.718618, acc: 0.000000]\n",
      "303: [D loss: 0.695593, acc: 0.601562]  [A loss: 0.657240, acc: 0.605469]\n",
      "304: [D loss: 1.307134, acc: 0.501953]  [A loss: 6.641701, acc: 0.000000]\n",
      "305: [D loss: 0.867524, acc: 0.585938]  [A loss: 0.035380, acc: 1.000000]\n",
      "306: [D loss: 2.350598, acc: 0.500000]  [A loss: 3.624067, acc: 0.000000]\n",
      "307: [D loss: 0.701554, acc: 0.605469]  [A loss: 2.422564, acc: 0.003906]\n",
      "308: [D loss: 1.118495, acc: 0.517578]  [A loss: 5.854814, acc: 0.000000]\n",
      "309: [D loss: 0.795479, acc: 0.580078]  [A loss: 0.082327, acc: 0.996094]\n",
      "310: [D loss: 2.002457, acc: 0.500000]  [A loss: 5.261977, acc: 0.000000]\n",
      "311: [D loss: 0.724576, acc: 0.589844]  [A loss: 0.105447, acc: 0.992188]\n",
      "312: [D loss: 1.845043, acc: 0.501953]  [A loss: 5.133128, acc: 0.000000]\n",
      "313: [D loss: 0.743229, acc: 0.609375]  [A loss: 0.083084, acc: 0.992188]\n",
      "314: [D loss: 1.898009, acc: 0.501953]  [A loss: 4.082128, acc: 0.000000]\n",
      "315: [D loss: 0.686114, acc: 0.601562]  [A loss: 0.584831, acc: 0.703125]\n",
      "316: [D loss: 1.284316, acc: 0.503906]  [A loss: 5.530346, acc: 0.000000]\n",
      "317: [D loss: 0.727454, acc: 0.601562]  [A loss: 0.136776, acc: 0.976562]\n",
      "318: [D loss: 1.851736, acc: 0.501953]  [A loss: 5.523088, acc: 0.000000]\n",
      "319: [D loss: 0.687900, acc: 0.642578]  [A loss: 0.117151, acc: 0.988281]\n",
      "320: [D loss: 1.623323, acc: 0.503906]  [A loss: 4.581845, acc: 0.000000]\n",
      "321: [D loss: 0.628909, acc: 0.644531]  [A loss: 0.584384, acc: 0.671875]\n",
      "322: [D loss: 1.353480, acc: 0.501953]  [A loss: 6.526215, acc: 0.000000]\n",
      "323: [D loss: 0.776653, acc: 0.613281]  [A loss: 0.053851, acc: 0.996094]\n",
      "324: [D loss: 2.183394, acc: 0.500000]  [A loss: 4.278248, acc: 0.000000]\n",
      "325: [D loss: 0.620869, acc: 0.666016]  [A loss: 0.712960, acc: 0.566406]\n",
      "326: [D loss: 1.361107, acc: 0.496094]  [A loss: 6.593970, acc: 0.000000]\n",
      "327: [D loss: 0.730046, acc: 0.615234]  [A loss: 0.063869, acc: 1.000000]\n",
      "328: [D loss: 1.981995, acc: 0.500000]  [A loss: 5.252521, acc: 0.000000]\n",
      "329: [D loss: 0.589896, acc: 0.687500]  [A loss: 0.813005, acc: 0.453125]\n",
      "330: [D loss: 1.549302, acc: 0.507812]  [A loss: 7.762765, acc: 0.000000]\n",
      "331: [D loss: 1.021153, acc: 0.585938]  [A loss: 0.009328, acc: 1.000000]\n",
      "332: [D loss: 3.382312, acc: 0.500000]  [A loss: 2.044463, acc: 0.019531]\n",
      "333: [D loss: 1.090812, acc: 0.519531]  [A loss: 4.966885, acc: 0.000000]\n",
      "334: [D loss: 0.644643, acc: 0.646484]  [A loss: 0.863835, acc: 0.480469]\n",
      "335: [D loss: 1.454413, acc: 0.505859]  [A loss: 7.111635, acc: 0.000000]\n",
      "336: [D loss: 0.841819, acc: 0.607422]  [A loss: 0.023019, acc: 1.000000]\n",
      "337: [D loss: 2.627432, acc: 0.500000]  [A loss: 4.298020, acc: 0.000000]\n",
      "338: [D loss: 0.749093, acc: 0.587891]  [A loss: 3.628753, acc: 0.000000]\n",
      "339: [D loss: 0.863628, acc: 0.542969]  [A loss: 5.554842, acc: 0.000000]\n",
      "340: [D loss: 0.696093, acc: 0.623047]  [A loss: 0.869816, acc: 0.437500]\n",
      "341: [D loss: 1.744013, acc: 0.496094]  [A loss: 9.899131, acc: 0.000000]\n",
      "342: [D loss: 1.508696, acc: 0.519531]  [A loss: 0.002520, acc: 1.000000]\n",
      "343: [D loss: 4.484450, acc: 0.500000]  [A loss: 1.205638, acc: 0.261719]\n",
      "344: [D loss: 1.477113, acc: 0.509766]  [A loss: 6.150459, acc: 0.000000]\n",
      "345: [D loss: 0.666821, acc: 0.642578]  [A loss: 0.378596, acc: 0.847656]\n",
      "346: [D loss: 1.582148, acc: 0.505859]  [A loss: 6.731898, acc: 0.000000]\n",
      "347: [D loss: 0.892416, acc: 0.603516]  [A loss: 0.039871, acc: 1.000000]\n",
      "348: [D loss: 2.278229, acc: 0.500000]  [A loss: 4.325489, acc: 0.000000]\n",
      "349: [D loss: 0.744890, acc: 0.591797]  [A loss: 2.595191, acc: 0.007812]\n",
      "350: [D loss: 1.151199, acc: 0.525391]  [A loss: 6.383280, acc: 0.000000]\n",
      "351: [D loss: 0.747394, acc: 0.623047]  [A loss: 0.114026, acc: 0.984375]\n",
      "352: [D loss: 1.974048, acc: 0.500000]  [A loss: 6.186510, acc: 0.000000]\n",
      "353: [D loss: 0.772648, acc: 0.595703]  [A loss: 0.112361, acc: 0.992188]\n",
      "354: [D loss: 1.825937, acc: 0.501953]  [A loss: 5.690606, acc: 0.000000]\n",
      "355: [D loss: 0.655513, acc: 0.650391]  [A loss: 0.158911, acc: 0.988281]\n",
      "356: [D loss: 1.631436, acc: 0.509766]  [A loss: 5.685297, acc: 0.000000]\n",
      "357: [D loss: 0.681315, acc: 0.646484]  [A loss: 0.165409, acc: 0.960938]\n",
      "358: [D loss: 1.656951, acc: 0.501953]  [A loss: 5.607969, acc: 0.000000]\n",
      "359: [D loss: 0.640355, acc: 0.673828]  [A loss: 0.412854, acc: 0.808594]\n",
      "360: [D loss: 1.552063, acc: 0.503906]  [A loss: 6.262991, acc: 0.000000]\n",
      "361: [D loss: 0.795670, acc: 0.632812]  [A loss: 0.026348, acc: 1.000000]\n",
      "362: [D loss: 2.596097, acc: 0.500000]  [A loss: 3.998437, acc: 0.000000]\n",
      "363: [D loss: 0.698314, acc: 0.613281]  [A loss: 3.339513, acc: 0.000000]\n",
      "364: [D loss: 1.048112, acc: 0.513672]  [A loss: 6.648198, acc: 0.000000]\n",
      "365: [D loss: 0.757359, acc: 0.605469]  [A loss: 0.104842, acc: 0.988281]\n",
      "366: [D loss: 2.026176, acc: 0.500000]  [A loss: 7.403380, acc: 0.000000]\n",
      "367: [D loss: 0.792935, acc: 0.617188]  [A loss: 0.028051, acc: 1.000000]\n",
      "368: [D loss: 2.581041, acc: 0.500000]  [A loss: 5.070908, acc: 0.000000]\n",
      "369: [D loss: 0.705425, acc: 0.613281]  [A loss: 1.983434, acc: 0.019531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370: [D loss: 1.468884, acc: 0.507812]  [A loss: 8.942386, acc: 0.000000]\n",
      "371: [D loss: 1.295164, acc: 0.542969]  [A loss: 0.004960, acc: 1.000000]\n",
      "372: [D loss: 4.106701, acc: 0.500000]  [A loss: 1.874595, acc: 0.054688]\n",
      "373: [D loss: 1.414032, acc: 0.529297]  [A loss: 5.848364, acc: 0.000000]\n",
      "374: [D loss: 0.679717, acc: 0.628906]  [A loss: 0.414316, acc: 0.800781]\n",
      "375: [D loss: 1.595871, acc: 0.503906]  [A loss: 6.791341, acc: 0.000000]\n",
      "376: [D loss: 0.801455, acc: 0.599609]  [A loss: 0.082034, acc: 0.996094]\n",
      "377: [D loss: 2.097315, acc: 0.503906]  [A loss: 5.127953, acc: 0.000000]\n",
      "378: [D loss: 0.708479, acc: 0.601562]  [A loss: 0.843938, acc: 0.488281]\n",
      "379: [D loss: 1.552530, acc: 0.501953]  [A loss: 7.213005, acc: 0.000000]\n",
      "380: [D loss: 0.823337, acc: 0.619141]  [A loss: 0.028600, acc: 1.000000]\n",
      "381: [D loss: 2.565025, acc: 0.500000]  [A loss: 4.603961, acc: 0.000000]\n",
      "382: [D loss: 0.698648, acc: 0.603516]  [A loss: 2.226964, acc: 0.023438]\n",
      "383: [D loss: 1.394021, acc: 0.513672]  [A loss: 7.669287, acc: 0.000000]\n",
      "384: [D loss: 0.918161, acc: 0.597656]  [A loss: 0.014189, acc: 1.000000]\n",
      "385: [D loss: 2.952832, acc: 0.500000]  [A loss: 4.978114, acc: 0.000000]\n",
      "386: [D loss: 0.763453, acc: 0.546875]  [A loss: 1.608267, acc: 0.152344]\n",
      "387: [D loss: 1.627239, acc: 0.509766]  [A loss: 8.721493, acc: 0.000000]\n",
      "388: [D loss: 1.110570, acc: 0.570312]  [A loss: 0.005197, acc: 1.000000]\n",
      "389: [D loss: 3.594679, acc: 0.500000]  [A loss: 3.126976, acc: 0.015625]\n",
      "390: [D loss: 1.187338, acc: 0.546875]  [A loss: 6.443462, acc: 0.000000]\n",
      "391: [D loss: 0.774402, acc: 0.603516]  [A loss: 0.244249, acc: 0.937500]\n",
      "392: [D loss: 1.776881, acc: 0.498047]  [A loss: 8.087619, acc: 0.000000]\n",
      "393: [D loss: 0.901092, acc: 0.617188]  [A loss: 0.019652, acc: 1.000000]\n",
      "394: [D loss: 2.924199, acc: 0.500000]  [A loss: 4.640204, acc: 0.000000]\n",
      "395: [D loss: 0.816273, acc: 0.572266]  [A loss: 3.933886, acc: 0.000000]\n",
      "396: [D loss: 1.042215, acc: 0.529297]  [A loss: 6.670238, acc: 0.000000]\n",
      "397: [D loss: 0.758647, acc: 0.611328]  [A loss: 0.396978, acc: 0.808594]\n",
      "398: [D loss: 1.792320, acc: 0.500000]  [A loss: 9.631691, acc: 0.000000]\n",
      "399: [D loss: 1.520550, acc: 0.527344]  [A loss: 0.002116, acc: 1.000000]\n",
      "400: [D loss: 4.209448, acc: 0.500000]  [A loss: 2.156498, acc: 0.066406]\n",
      "401: [D loss: 1.416618, acc: 0.529297]  [A loss: 6.483036, acc: 0.000000]\n",
      "402: [D loss: 0.707034, acc: 0.646484]  [A loss: 0.489246, acc: 0.761719]\n",
      "403: [D loss: 1.612275, acc: 0.496094]  [A loss: 7.530191, acc: 0.000000]\n",
      "404: [D loss: 0.879439, acc: 0.591797]  [A loss: 0.025798, acc: 1.000000]\n",
      "405: [D loss: 2.719852, acc: 0.500000]  [A loss: 5.684764, acc: 0.000000]\n",
      "406: [D loss: 0.708982, acc: 0.623047]  [A loss: 1.120931, acc: 0.320312]\n",
      "407: [D loss: 1.687021, acc: 0.501953]  [A loss: 8.111584, acc: 0.000000]\n",
      "408: [D loss: 0.992742, acc: 0.566406]  [A loss: 0.015114, acc: 1.000000]\n",
      "409: [D loss: 2.872362, acc: 0.500000]  [A loss: 5.018579, acc: 0.000000]\n",
      "410: [D loss: 0.783642, acc: 0.566406]  [A loss: 2.972526, acc: 0.003906]\n",
      "411: [D loss: 1.360369, acc: 0.505859]  [A loss: 8.439570, acc: 0.000000]\n",
      "412: [D loss: 1.010924, acc: 0.570312]  [A loss: 0.011334, acc: 1.000000]\n",
      "413: [D loss: 3.334210, acc: 0.500000]  [A loss: 4.558353, acc: 0.000000]\n",
      "414: [D loss: 0.872015, acc: 0.568359]  [A loss: 3.928488, acc: 0.000000]\n",
      "415: [D loss: 1.113338, acc: 0.529297]  [A loss: 6.599877, acc: 0.000000]\n",
      "416: [D loss: 0.787251, acc: 0.607422]  [A loss: 0.125171, acc: 0.980469]\n",
      "417: [D loss: 1.958317, acc: 0.503906]  [A loss: 8.432014, acc: 0.000000]\n",
      "418: [D loss: 1.054628, acc: 0.568359]  [A loss: 0.007013, acc: 1.000000]\n",
      "419: [D loss: 3.256077, acc: 0.500000]  [A loss: 4.027228, acc: 0.000000]\n",
      "420: [D loss: 0.853136, acc: 0.550781]  [A loss: 3.578528, acc: 0.000000]\n",
      "421: [D loss: 1.176053, acc: 0.503906]  [A loss: 7.348443, acc: 0.000000]\n",
      "422: [D loss: 0.829443, acc: 0.580078]  [A loss: 0.048889, acc: 0.996094]\n",
      "423: [D loss: 2.446568, acc: 0.503906]  [A loss: 6.937566, acc: 0.000000]\n",
      "424: [D loss: 0.777408, acc: 0.628906]  [A loss: 0.121664, acc: 0.988281]\n",
      "425: [D loss: 1.878839, acc: 0.505859]  [A loss: 7.725153, acc: 0.000000]\n",
      "426: [D loss: 0.974498, acc: 0.593750]  [A loss: 0.015818, acc: 1.000000]\n",
      "427: [D loss: 2.949296, acc: 0.501953]  [A loss: 4.454735, acc: 0.000000]\n",
      "428: [D loss: 0.817129, acc: 0.578125]  [A loss: 3.159338, acc: 0.011719]\n",
      "429: [D loss: 1.193043, acc: 0.523438]  [A loss: 6.867649, acc: 0.000000]\n",
      "430: [D loss: 0.832895, acc: 0.585938]  [A loss: 0.174104, acc: 0.953125]\n",
      "431: [D loss: 1.847602, acc: 0.501953]  [A loss: 8.224198, acc: 0.000000]\n",
      "432: [D loss: 1.094818, acc: 0.558594]  [A loss: 0.007567, acc: 1.000000]\n",
      "433: [D loss: 3.550048, acc: 0.500000]  [A loss: 4.155147, acc: 0.000000]\n",
      "434: [D loss: 0.952446, acc: 0.548828]  [A loss: 5.509188, acc: 0.000000]\n",
      "435: [D loss: 0.822696, acc: 0.552734]  [A loss: 3.087517, acc: 0.003906]\n",
      "436: [D loss: 1.560872, acc: 0.501953]  [A loss: 9.969892, acc: 0.000000]\n",
      "437: [D loss: 1.308396, acc: 0.542969]  [A loss: 0.001547, acc: 1.000000]\n",
      "438: [D loss: 4.795586, acc: 0.500000]  [A loss: 2.876583, acc: 0.011719]\n",
      "439: [D loss: 1.443471, acc: 0.513672]  [A loss: 7.604985, acc: 0.000000]\n",
      "440: [D loss: 0.914605, acc: 0.589844]  [A loss: 0.047120, acc: 0.992188]\n",
      "441: [D loss: 2.473225, acc: 0.500000]  [A loss: 5.924248, acc: 0.000000]\n",
      "442: [D loss: 0.762730, acc: 0.582031]  [A loss: 0.576081, acc: 0.687500]\n",
      "443: [D loss: 1.789842, acc: 0.498047]  [A loss: 7.999564, acc: 0.000000]\n",
      "444: [D loss: 1.015117, acc: 0.570312]  [A loss: 0.010033, acc: 1.000000]\n",
      "445: [D loss: 3.134789, acc: 0.500000]  [A loss: 4.033145, acc: 0.000000]\n",
      "446: [D loss: 0.955859, acc: 0.562500]  [A loss: 5.211039, acc: 0.000000]\n",
      "447: [D loss: 0.791614, acc: 0.597656]  [A loss: 2.781394, acc: 0.019531]\n",
      "448: [D loss: 1.588296, acc: 0.500000]  [A loss: 9.484432, acc: 0.000000]\n",
      "449: [D loss: 1.141183, acc: 0.546875]  [A loss: 0.014415, acc: 1.000000]\n",
      "450: [D loss: 3.645668, acc: 0.500000]  [A loss: 5.125776, acc: 0.000000]\n",
      "451: [D loss: 0.785326, acc: 0.595703]  [A loss: 3.360142, acc: 0.003906]\n",
      "452: [D loss: 1.353380, acc: 0.531250]  [A loss: 9.086873, acc: 0.000000]\n",
      "453: [D loss: 1.208729, acc: 0.548828]  [A loss: 0.004551, acc: 1.000000]\n",
      "454: [D loss: 4.067197, acc: 0.500000]  [A loss: 3.434402, acc: 0.011719]\n",
      "455: [D loss: 1.417302, acc: 0.539062]  [A loss: 7.197554, acc: 0.000000]\n",
      "456: [D loss: 0.869492, acc: 0.574219]  [A loss: 0.187752, acc: 0.941406]\n",
      "457: [D loss: 1.869040, acc: 0.509766]  [A loss: 7.637003, acc: 0.000000]\n",
      "458: [D loss: 0.915952, acc: 0.593750]  [A loss: 0.034374, acc: 0.996094]\n",
      "459: [D loss: 2.400562, acc: 0.500000]  [A loss: 5.643399, acc: 0.000000]\n",
      "460: [D loss: 0.727816, acc: 0.628906]  [A loss: 0.611454, acc: 0.652344]\n",
      "461: [D loss: 1.711426, acc: 0.501953]  [A loss: 7.916164, acc: 0.000000]\n",
      "462: [D loss: 1.027536, acc: 0.576172]  [A loss: 0.017588, acc: 1.000000]\n",
      "463: [D loss: 2.827646, acc: 0.500000]  [A loss: 4.763803, acc: 0.000000]\n",
      "464: [D loss: 0.884359, acc: 0.582031]  [A loss: 4.769013, acc: 0.000000]\n",
      "465: [D loss: 0.878643, acc: 0.556641]  [A loss: 3.993316, acc: 0.000000]\n",
      "466: [D loss: 1.187894, acc: 0.515625]  [A loss: 8.805062, acc: 0.000000]\n",
      "467: [D loss: 1.092997, acc: 0.541016]  [A loss: 0.008479, acc: 1.000000]\n",
      "468: [D loss: 3.542347, acc: 0.500000]  [A loss: 5.340921, acc: 0.000000]\n",
      "469: [D loss: 0.830239, acc: 0.568359]  [A loss: 3.605822, acc: 0.000000]\n",
      "470: [D loss: 1.355345, acc: 0.521484]  [A loss: 10.008568, acc: 0.000000]\n",
      "471: [D loss: 1.465249, acc: 0.529297]  [A loss: 0.001755, acc: 1.000000]\n",
      "472: [D loss: 4.695074, acc: 0.500000]  [A loss: 2.504821, acc: 0.054688]\n",
      "473: [D loss: 1.620272, acc: 0.498047]  [A loss: 8.288260, acc: 0.000000]\n",
      "474: [D loss: 1.037168, acc: 0.562500]  [A loss: 0.015996, acc: 0.996094]\n",
      "475: [D loss: 3.311582, acc: 0.500000]  [A loss: 5.199065, acc: 0.003906]\n",
      "476: [D loss: 0.908660, acc: 0.558594]  [A loss: 5.309234, acc: 0.000000]\n",
      "477: [D loss: 0.895767, acc: 0.542969]  [A loss: 4.843263, acc: 0.000000]\n",
      "478: [D loss: 1.093692, acc: 0.500000]  [A loss: 7.725863, acc: 0.000000]\n",
      "479: [D loss: 0.943245, acc: 0.583984]  [A loss: 0.059734, acc: 0.996094]\n",
      "480: [D loss: 2.524519, acc: 0.507812]  [A loss: 8.949234, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481: [D loss: 1.168378, acc: 0.546875]  [A loss: 0.007435, acc: 1.000000]\n",
      "482: [D loss: 4.019090, acc: 0.500000]  [A loss: 3.822124, acc: 0.003906]\n",
      "483: [D loss: 1.051565, acc: 0.554688]  [A loss: 6.287003, acc: 0.000000]\n",
      "484: [D loss: 0.750531, acc: 0.593750]  [A loss: 0.595524, acc: 0.683594]\n",
      "485: [D loss: 1.692777, acc: 0.501953]  [A loss: 9.322035, acc: 0.000000]\n",
      "486: [D loss: 1.317003, acc: 0.550781]  [A loss: 0.005195, acc: 1.000000]\n",
      "487: [D loss: 4.208699, acc: 0.500000]  [A loss: 3.454854, acc: 0.015625]\n",
      "488: [D loss: 1.316647, acc: 0.529297]  [A loss: 6.769234, acc: 0.000000]\n",
      "489: [D loss: 0.788510, acc: 0.582031]  [A loss: 1.027893, acc: 0.375000]\n",
      "490: [D loss: 1.912511, acc: 0.498047]  [A loss: 9.033949, acc: 0.000000]\n",
      "491: [D loss: 1.238500, acc: 0.539062]  [A loss: 0.004582, acc: 1.000000]\n",
      "492: [D loss: 4.106145, acc: 0.500000]  [A loss: 3.605864, acc: 0.003906]\n",
      "493: [D loss: 1.365711, acc: 0.527344]  [A loss: 7.387376, acc: 0.000000]\n",
      "494: [D loss: 0.774306, acc: 0.615234]  [A loss: 0.194340, acc: 0.941406]\n",
      "495: [D loss: 1.849793, acc: 0.500000]  [A loss: 7.688218, acc: 0.000000]\n",
      "496: [D loss: 0.940767, acc: 0.564453]  [A loss: 0.062556, acc: 0.996094]\n",
      "497: [D loss: 2.174537, acc: 0.509766]  [A loss: 6.630012, acc: 0.000000]\n",
      "498: [D loss: 0.732329, acc: 0.636719]  [A loss: 0.432010, acc: 0.789062]\n",
      "499: [D loss: 1.637854, acc: 0.501953]  [A loss: 8.219979, acc: 0.000000]\n",
      "500: [D loss: 0.921145, acc: 0.591797]  [A loss: 0.033451, acc: 1.000000]\n",
      "501: [D loss: 2.641039, acc: 0.501953]  [A loss: 5.658332, acc: 0.000000]\n",
      "502: [D loss: 0.703502, acc: 0.628906]  [A loss: 1.921366, acc: 0.113281]\n",
      "503: [D loss: 1.774143, acc: 0.503906]  [A loss: 9.100494, acc: 0.000000]\n",
      "504: [D loss: 0.992467, acc: 0.603516]  [A loss: 0.014693, acc: 1.000000]\n",
      "505: [D loss: 3.422786, acc: 0.500000]  [A loss: 5.685540, acc: 0.000000]\n",
      "506: [D loss: 0.887411, acc: 0.599609]  [A loss: 4.640394, acc: 0.000000]\n",
      "507: [D loss: 1.200847, acc: 0.501953]  [A loss: 7.982549, acc: 0.000000]\n",
      "508: [D loss: 0.972356, acc: 0.578125]  [A loss: 0.033354, acc: 0.996094]\n",
      "509: [D loss: 3.182182, acc: 0.500000]  [A loss: 7.693720, acc: 0.000000]\n",
      "510: [D loss: 0.791066, acc: 0.591797]  [A loss: 0.257792, acc: 0.914062]\n",
      "511: [D loss: 1.809420, acc: 0.503906]  [A loss: 9.151209, acc: 0.000000]\n",
      "512: [D loss: 1.078564, acc: 0.576172]  [A loss: 0.005154, acc: 1.000000]\n",
      "513: [D loss: 3.864602, acc: 0.500000]  [A loss: 3.594667, acc: 0.003906]\n",
      "514: [D loss: 1.274544, acc: 0.541016]  [A loss: 7.442950, acc: 0.000000]\n",
      "515: [D loss: 0.762911, acc: 0.621094]  [A loss: 0.200118, acc: 0.937500]\n",
      "516: [D loss: 2.003495, acc: 0.501953]  [A loss: 8.734938, acc: 0.000000]\n",
      "517: [D loss: 0.966510, acc: 0.580078]  [A loss: 0.021090, acc: 1.000000]\n",
      "518: [D loss: 2.788813, acc: 0.500000]  [A loss: 5.734253, acc: 0.000000]\n",
      "519: [D loss: 0.771632, acc: 0.603516]  [A loss: 4.132944, acc: 0.000000]\n",
      "520: [D loss: 1.285203, acc: 0.521484]  [A loss: 8.729820, acc: 0.000000]\n",
      "521: [D loss: 1.122495, acc: 0.572266]  [A loss: 0.006182, acc: 1.000000]\n",
      "522: [D loss: 3.628400, acc: 0.500000]  [A loss: 5.413321, acc: 0.000000]\n",
      "523: [D loss: 0.974458, acc: 0.589844]  [A loss: 4.862331, acc: 0.000000]\n",
      "524: [D loss: 1.070260, acc: 0.539062]  [A loss: 7.621412, acc: 0.000000]\n",
      "525: [D loss: 0.790776, acc: 0.601562]  [A loss: 0.460870, acc: 0.777344]\n",
      "526: [D loss: 2.162186, acc: 0.505859]  [A loss: 12.095654, acc: 0.000000]\n",
      "527: [D loss: 2.178049, acc: 0.507812]  [A loss: 0.000270, acc: 1.000000]\n",
      "528: [D loss: 5.860292, acc: 0.500000]  [A loss: 1.353555, acc: 0.308594]\n",
      "529: [D loss: 1.918509, acc: 0.503906]  [A loss: 8.675985, acc: 0.000000]\n",
      "530: [D loss: 0.881054, acc: 0.589844]  [A loss: 0.115357, acc: 0.976562]\n",
      "531: [D loss: 2.160459, acc: 0.503906]  [A loss: 7.584328, acc: 0.000000]\n",
      "532: [D loss: 0.809698, acc: 0.646484]  [A loss: 0.127546, acc: 0.976562]\n",
      "533: [D loss: 2.115015, acc: 0.503906]  [A loss: 7.480427, acc: 0.000000]\n",
      "534: [D loss: 0.779294, acc: 0.630859]  [A loss: 0.110988, acc: 0.972656]\n",
      "535: [D loss: 2.237355, acc: 0.500000]  [A loss: 7.069505, acc: 0.000000]\n",
      "536: [D loss: 0.749594, acc: 0.611328]  [A loss: 0.295597, acc: 0.871094]\n",
      "537: [D loss: 1.745253, acc: 0.505859]  [A loss: 8.158268, acc: 0.000000]\n",
      "538: [D loss: 0.866101, acc: 0.583984]  [A loss: 0.049183, acc: 0.996094]\n",
      "539: [D loss: 2.608552, acc: 0.503906]  [A loss: 6.833805, acc: 0.000000]\n",
      "540: [D loss: 0.732278, acc: 0.615234]  [A loss: 0.885338, acc: 0.511719]\n",
      "541: [D loss: 1.866235, acc: 0.501953]  [A loss: 9.315233, acc: 0.000000]\n",
      "542: [D loss: 0.983605, acc: 0.603516]  [A loss: 0.010105, acc: 1.000000]\n",
      "543: [D loss: 3.411037, acc: 0.500000]  [A loss: 6.115685, acc: 0.000000]\n",
      "544: [D loss: 0.808466, acc: 0.591797]  [A loss: 4.135820, acc: 0.000000]\n",
      "545: [D loss: 1.370923, acc: 0.529297]  [A loss: 9.378771, acc: 0.000000]\n",
      "546: [D loss: 1.099890, acc: 0.548828]  [A loss: 0.006378, acc: 1.000000]\n",
      "547: [D loss: 3.814327, acc: 0.500000]  [A loss: 5.463501, acc: 0.000000]\n",
      "548: [D loss: 1.001506, acc: 0.566406]  [A loss: 6.110178, acc: 0.000000]\n",
      "549: [D loss: 0.844319, acc: 0.593750]  [A loss: 4.397390, acc: 0.000000]\n",
      "550: [D loss: 1.423166, acc: 0.525391]  [A loss: 10.836311, acc: 0.000000]\n",
      "551: [D loss: 1.497046, acc: 0.523438]  [A loss: 0.000990, acc: 1.000000]\n",
      "552: [D loss: 5.419794, acc: 0.500000]  [A loss: 2.891209, acc: 0.019531]\n",
      "553: [D loss: 2.016124, acc: 0.496094]  [A loss: 10.933585, acc: 0.000000]\n",
      "554: [D loss: 1.307805, acc: 0.564453]  [A loss: 0.003342, acc: 1.000000]\n",
      "555: [D loss: 4.572990, acc: 0.500000]  [A loss: 4.097442, acc: 0.015625]\n",
      "556: [D loss: 1.409299, acc: 0.535156]  [A loss: 8.195941, acc: 0.000000]\n",
      "557: [D loss: 0.915288, acc: 0.568359]  [A loss: 0.196665, acc: 0.933594]\n",
      "558: [D loss: 1.933265, acc: 0.500000]  [A loss: 8.761162, acc: 0.000000]\n",
      "559: [D loss: 1.007000, acc: 0.591797]  [A loss: 0.023653, acc: 1.000000]\n",
      "560: [D loss: 3.172222, acc: 0.500000]  [A loss: 6.521163, acc: 0.000000]\n",
      "561: [D loss: 0.790715, acc: 0.572266]  [A loss: 2.127984, acc: 0.109375]\n",
      "562: [D loss: 1.854856, acc: 0.511719]  [A loss: 9.904997, acc: 0.000000]\n",
      "563: [D loss: 1.119289, acc: 0.541016]  [A loss: 0.004088, acc: 1.000000]\n",
      "564: [D loss: 3.835029, acc: 0.500000]  [A loss: 5.260369, acc: 0.003906]\n",
      "565: [D loss: 1.050300, acc: 0.541016]  [A loss: 6.637703, acc: 0.000000]\n",
      "566: [D loss: 0.821478, acc: 0.582031]  [A loss: 2.147638, acc: 0.148438]\n",
      "567: [D loss: 2.169710, acc: 0.500000]  [A loss: 11.848486, acc: 0.000000]\n",
      "568: [D loss: 1.667458, acc: 0.501953]  [A loss: 0.001656, acc: 1.000000]\n",
      "569: [D loss: 5.532690, acc: 0.500000]  [A loss: 3.272502, acc: 0.011719]\n",
      "570: [D loss: 1.935428, acc: 0.513672]  [A loss: 9.821296, acc: 0.000000]\n",
      "571: [D loss: 1.207221, acc: 0.541016]  [A loss: 0.014249, acc: 1.000000]\n",
      "572: [D loss: 3.738078, acc: 0.500000]  [A loss: 7.008213, acc: 0.000000]\n",
      "573: [D loss: 0.870126, acc: 0.564453]  [A loss: 3.253131, acc: 0.027344]\n",
      "574: [D loss: 2.005128, acc: 0.511719]  [A loss: 10.742214, acc: 0.000000]\n",
      "575: [D loss: 1.560426, acc: 0.537109]  [A loss: 0.001564, acc: 1.000000]\n",
      "576: [D loss: 5.098122, acc: 0.500000]  [A loss: 4.068256, acc: 0.007812]\n",
      "577: [D loss: 1.858748, acc: 0.486328]  [A loss: 9.256408, acc: 0.000000]\n",
      "578: [D loss: 1.057701, acc: 0.562500]  [A loss: 0.023524, acc: 1.000000]\n",
      "579: [D loss: 3.255786, acc: 0.501953]  [A loss: 7.559359, acc: 0.000000]\n",
      "580: [D loss: 0.842493, acc: 0.564453]  [A loss: 0.824732, acc: 0.539062]\n",
      "581: [D loss: 2.108109, acc: 0.496094]  [A loss: 9.693140, acc: 0.000000]\n",
      "582: [D loss: 1.074242, acc: 0.542969]  [A loss: 0.014664, acc: 0.996094]\n",
      "583: [D loss: 3.380711, acc: 0.500000]  [A loss: 6.759713, acc: 0.000000]\n",
      "584: [D loss: 0.816603, acc: 0.587891]  [A loss: 1.755809, acc: 0.175781]\n",
      "585: [D loss: 2.118768, acc: 0.494141]  [A loss: 10.441887, acc: 0.000000]\n",
      "586: [D loss: 1.392235, acc: 0.515625]  [A loss: 0.002440, acc: 1.000000]\n",
      "587: [D loss: 4.733188, acc: 0.500000]  [A loss: 4.620606, acc: 0.000000]\n",
      "588: [D loss: 1.302746, acc: 0.523438]  [A loss: 7.562322, acc: 0.000000]\n",
      "589: [D loss: 0.864165, acc: 0.564453]  [A loss: 1.004843, acc: 0.437500]\n",
      "590: [D loss: 2.212365, acc: 0.501953]  [A loss: 10.766294, acc: 0.000000]\n",
      "591: [D loss: 1.419521, acc: 0.544922]  [A loss: 0.005599, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592: [D loss: 4.042895, acc: 0.500000]  [A loss: 5.484823, acc: 0.000000]\n",
      "593: [D loss: 1.021010, acc: 0.523438]  [A loss: 4.784222, acc: 0.000000]\n",
      "594: [D loss: 1.303513, acc: 0.500000]  [A loss: 8.006723, acc: 0.000000]\n",
      "595: [D loss: 0.871951, acc: 0.593750]  [A loss: 0.137677, acc: 0.945312]\n",
      "596: [D loss: 2.302159, acc: 0.509766]  [A loss: 9.848890, acc: 0.000000]\n",
      "597: [D loss: 1.225886, acc: 0.537109]  [A loss: 0.007611, acc: 1.000000]\n",
      "598: [D loss: 3.648648, acc: 0.500000]  [A loss: 5.283553, acc: 0.000000]\n",
      "599: [D loss: 0.946318, acc: 0.564453]  [A loss: 5.676041, acc: 0.000000]\n",
      "600: [D loss: 0.977860, acc: 0.566406]  [A loss: 5.867281, acc: 0.000000]\n",
      "601: [D loss: 1.006917, acc: 0.523438]  [A loss: 5.984317, acc: 0.000000]\n",
      "602: [D loss: 1.057338, acc: 0.556641]  [A loss: 6.200035, acc: 0.000000]\n",
      "603: [D loss: 1.085588, acc: 0.535156]  [A loss: 7.970175, acc: 0.000000]\n",
      "604: [D loss: 0.983300, acc: 0.533203]  [A loss: 1.049197, acc: 0.460938]\n",
      "605: [D loss: 2.153307, acc: 0.505859]  [A loss: 14.537268, acc: 0.000000]\n",
      "606: [D loss: 5.253971, acc: 0.500000]  [A loss: 0.002194, acc: 1.000000]\n",
      "607: [D loss: 5.243578, acc: 0.500000]  [A loss: 4.112617, acc: 0.015625]\n",
      "608: [D loss: 2.029078, acc: 0.517578]  [A loss: 10.788220, acc: 0.000000]\n",
      "609: [D loss: 1.302000, acc: 0.531250]  [A loss: 0.009220, acc: 1.000000]\n",
      "610: [D loss: 4.403468, acc: 0.500000]  [A loss: 5.254909, acc: 0.000000]\n",
      "611: [D loss: 1.141099, acc: 0.564453]  [A loss: 7.086424, acc: 0.000000]\n",
      "612: [D loss: 0.854557, acc: 0.564453]  [A loss: 1.514484, acc: 0.320312]\n",
      "613: [D loss: 1.958423, acc: 0.515625]  [A loss: 10.240437, acc: 0.000000]\n",
      "614: [D loss: 1.374835, acc: 0.556641]  [A loss: 0.004969, acc: 1.000000]\n",
      "615: [D loss: 4.459941, acc: 0.500000]  [A loss: 5.437122, acc: 0.003906]\n",
      "616: [D loss: 1.106072, acc: 0.566406]  [A loss: 6.851116, acc: 0.000000]\n",
      "617: [D loss: 0.910417, acc: 0.570312]  [A loss: 2.183392, acc: 0.113281]\n",
      "618: [D loss: 2.264150, acc: 0.505859]  [A loss: 12.094709, acc: 0.000000]\n",
      "619: [D loss: 2.050369, acc: 0.509766]  [A loss: 0.001242, acc: 1.000000]\n",
      "620: [D loss: 5.897374, acc: 0.500000]  [A loss: 1.506017, acc: 0.292969]\n",
      "621: [D loss: 2.267327, acc: 0.496094]  [A loss: 10.129293, acc: 0.000000]\n",
      "622: [D loss: 1.224632, acc: 0.558594]  [A loss: 0.028922, acc: 1.000000]\n",
      "623: [D loss: 3.372772, acc: 0.501953]  [A loss: 7.140746, acc: 0.000000]\n",
      "624: [D loss: 0.890141, acc: 0.558594]  [A loss: 2.140188, acc: 0.113281]\n",
      "625: [D loss: 1.949943, acc: 0.503906]  [A loss: 9.909131, acc: 0.000000]\n",
      "626: [D loss: 1.104565, acc: 0.582031]  [A loss: 0.016118, acc: 0.996094]\n",
      "627: [D loss: 3.582304, acc: 0.501953]  [A loss: 6.317087, acc: 0.000000]\n",
      "628: [D loss: 0.918007, acc: 0.568359]  [A loss: 3.627707, acc: 0.011719]\n",
      "629: [D loss: 1.747984, acc: 0.507812]  [A loss: 10.099443, acc: 0.000000]\n",
      "630: [D loss: 1.391811, acc: 0.525391]  [A loss: 0.019548, acc: 1.000000]\n",
      "631: [D loss: 3.774217, acc: 0.500000]  [A loss: 6.264110, acc: 0.000000]\n",
      "632: [D loss: 1.252252, acc: 0.525391]  [A loss: 6.645570, acc: 0.000000]\n",
      "633: [D loss: 0.913900, acc: 0.568359]  [A loss: 4.144598, acc: 0.015625]\n",
      "634: [D loss: 1.670793, acc: 0.523438]  [A loss: 11.176777, acc: 0.000000]\n",
      "635: [D loss: 1.621224, acc: 0.527344]  [A loss: 0.000866, acc: 1.000000]\n",
      "636: [D loss: 5.709046, acc: 0.500000]  [A loss: 1.567009, acc: 0.269531]\n",
      "637: [D loss: 2.103957, acc: 0.503906]  [A loss: 10.699459, acc: 0.000000]\n",
      "638: [D loss: 1.167009, acc: 0.572266]  [A loss: 0.022110, acc: 0.992188]\n",
      "639: [D loss: 3.906533, acc: 0.500000]  [A loss: 6.286403, acc: 0.000000]\n",
      "640: [D loss: 1.077140, acc: 0.570312]  [A loss: 7.067751, acc: 0.000000]\n",
      "641: [D loss: 0.868021, acc: 0.591797]  [A loss: 2.521057, acc: 0.089844]\n",
      "642: [D loss: 2.099629, acc: 0.509766]  [A loss: 11.562841, acc: 0.000000]\n",
      "643: [D loss: 2.053817, acc: 0.525391]  [A loss: 0.001806, acc: 1.000000]\n",
      "644: [D loss: 5.610055, acc: 0.500000]  [A loss: 1.409114, acc: 0.339844]\n",
      "645: [D loss: 2.443749, acc: 0.492188]  [A loss: 9.762474, acc: 0.000000]\n",
      "646: [D loss: 1.127654, acc: 0.568359]  [A loss: 0.019201, acc: 1.000000]\n",
      "647: [D loss: 3.247853, acc: 0.501953]  [A loss: 5.864030, acc: 0.003906]\n",
      "648: [D loss: 1.018661, acc: 0.599609]  [A loss: 5.032993, acc: 0.000000]\n",
      "649: [D loss: 1.360372, acc: 0.554688]  [A loss: 8.372223, acc: 0.000000]\n",
      "650: [D loss: 1.000611, acc: 0.570312]  [A loss: 0.171973, acc: 0.937500]\n",
      "651: [D loss: 2.221770, acc: 0.515625]  [A loss: 10.107009, acc: 0.000000]\n",
      "652: [D loss: 1.330414, acc: 0.548828]  [A loss: 0.010596, acc: 1.000000]\n",
      "653: [D loss: 4.441892, acc: 0.500000]  [A loss: 4.423470, acc: 0.011719]\n",
      "654: [D loss: 1.575654, acc: 0.541016]  [A loss: 8.601072, acc: 0.000000]\n",
      "655: [D loss: 0.969537, acc: 0.580078]  [A loss: 0.084973, acc: 0.972656]\n",
      "656: [D loss: 2.326801, acc: 0.505859]  [A loss: 7.388267, acc: 0.000000]\n",
      "657: [D loss: 0.784312, acc: 0.652344]  [A loss: 0.289475, acc: 0.851562]\n",
      "658: [D loss: 1.971608, acc: 0.519531]  [A loss: 8.732162, acc: 0.000000]\n",
      "659: [D loss: 1.097276, acc: 0.566406]  [A loss: 0.043049, acc: 0.984375]\n",
      "660: [D loss: 3.355756, acc: 0.501953]  [A loss: 7.066303, acc: 0.000000]\n",
      "661: [D loss: 0.824008, acc: 0.609375]  [A loss: 3.281266, acc: 0.011719]\n",
      "662: [D loss: 1.932898, acc: 0.511719]  [A loss: 10.213982, acc: 0.000000]\n",
      "663: [D loss: 1.356682, acc: 0.541016]  [A loss: 0.012246, acc: 0.996094]\n",
      "664: [D loss: 4.565704, acc: 0.501953]  [A loss: 2.791869, acc: 0.050781]\n",
      "665: [D loss: 1.810702, acc: 0.527344]  [A loss: 9.021903, acc: 0.000000]\n",
      "666: [D loss: 1.036892, acc: 0.562500]  [A loss: 0.053494, acc: 0.992188]\n",
      "667: [D loss: 2.915563, acc: 0.501953]  [A loss: 7.505810, acc: 0.000000]\n",
      "668: [D loss: 0.751463, acc: 0.654297]  [A loss: 0.623941, acc: 0.664062]\n",
      "669: [D loss: 1.761628, acc: 0.509766]  [A loss: 8.392365, acc: 0.000000]\n",
      "670: [D loss: 0.974947, acc: 0.572266]  [A loss: 0.086133, acc: 0.953125]\n",
      "671: [D loss: 2.884051, acc: 0.505859]  [A loss: 7.902776, acc: 0.000000]\n",
      "672: [D loss: 0.821745, acc: 0.615234]  [A loss: 1.958537, acc: 0.167969]\n",
      "673: [D loss: 1.864032, acc: 0.519531]  [A loss: 9.763443, acc: 0.000000]\n",
      "674: [D loss: 1.087812, acc: 0.556641]  [A loss: 0.011216, acc: 1.000000]\n",
      "675: [D loss: 3.703196, acc: 0.500000]  [A loss: 5.955508, acc: 0.003906]\n",
      "676: [D loss: 0.902883, acc: 0.593750]  [A loss: 3.947448, acc: 0.007812]\n",
      "677: [D loss: 1.367094, acc: 0.535156]  [A loss: 8.951580, acc: 0.000000]\n",
      "678: [D loss: 1.077086, acc: 0.548828]  [A loss: 0.021334, acc: 0.992188]\n",
      "679: [D loss: 3.169012, acc: 0.500000]  [A loss: 7.213658, acc: 0.000000]\n",
      "680: [D loss: 0.809207, acc: 0.617188]  [A loss: 0.859128, acc: 0.550781]\n",
      "681: [D loss: 2.114424, acc: 0.513672]  [A loss: 10.085028, acc: 0.000000]\n",
      "682: [D loss: 1.403505, acc: 0.548828]  [A loss: 0.002607, acc: 1.000000]\n",
      "683: [D loss: 4.826197, acc: 0.500000]  [A loss: 3.399268, acc: 0.019531]\n",
      "684: [D loss: 1.830977, acc: 0.519531]  [A loss: 9.001610, acc: 0.000000]\n",
      "685: [D loss: 0.863455, acc: 0.605469]  [A loss: 0.083072, acc: 0.976562]\n",
      "686: [D loss: 2.524319, acc: 0.501953]  [A loss: 7.443259, acc: 0.000000]\n",
      "687: [D loss: 0.712426, acc: 0.650391]  [A loss: 0.523720, acc: 0.738281]\n",
      "688: [D loss: 1.851416, acc: 0.513672]  [A loss: 8.563319, acc: 0.000000]\n",
      "689: [D loss: 1.042449, acc: 0.574219]  [A loss: 0.046107, acc: 0.984375]\n",
      "690: [D loss: 3.095568, acc: 0.501953]  [A loss: 6.544440, acc: 0.000000]\n",
      "691: [D loss: 0.868528, acc: 0.591797]  [A loss: 4.509444, acc: 0.003906]\n",
      "692: [D loss: 1.346260, acc: 0.554688]  [A loss: 8.817967, acc: 0.000000]\n",
      "693: [D loss: 0.880775, acc: 0.576172]  [A loss: 0.057962, acc: 0.988281]\n",
      "694: [D loss: 2.692299, acc: 0.501953]  [A loss: 9.208709, acc: 0.000000]\n",
      "695: [D loss: 1.201589, acc: 0.554688]  [A loss: 0.019606, acc: 1.000000]\n",
      "696: [D loss: 3.683398, acc: 0.500000]  [A loss: 5.446941, acc: 0.000000]\n",
      "697: [D loss: 1.262579, acc: 0.576172]  [A loss: 8.518827, acc: 0.000000]\n",
      "698: [D loss: 0.863418, acc: 0.607422]  [A loss: 0.095523, acc: 0.972656]\n",
      "699: [D loss: 2.518830, acc: 0.501953]  [A loss: 8.307613, acc: 0.000000]\n",
      "700: [D loss: 0.860203, acc: 0.615234]  [A loss: 0.057094, acc: 0.992188]\n",
      "701: [D loss: 2.636604, acc: 0.501953]  [A loss: 6.882396, acc: 0.000000]\n",
      "702: [D loss: 0.783974, acc: 0.619141]  [A loss: 1.482494, acc: 0.253906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "703: [D loss: 1.929061, acc: 0.511719]  [A loss: 10.269342, acc: 0.000000]\n",
      "704: [D loss: 1.215729, acc: 0.558594]  [A loss: 0.007983, acc: 1.000000]\n",
      "705: [D loss: 4.536678, acc: 0.500000]  [A loss: 6.194008, acc: 0.000000]\n",
      "706: [D loss: 1.101581, acc: 0.576172]  [A loss: 7.066760, acc: 0.000000]\n",
      "707: [D loss: 0.869638, acc: 0.582031]  [A loss: 3.475508, acc: 0.011719]\n",
      "708: [D loss: 2.047473, acc: 0.511719]  [A loss: 12.709700, acc: 0.000000]\n",
      "709: [D loss: 2.607450, acc: 0.513672]  [A loss: 0.001786, acc: 1.000000]\n",
      "710: [D loss: 6.159762, acc: 0.500000]  [A loss: 0.697981, acc: 0.675781]\n",
      "711: [D loss: 2.236936, acc: 0.517578]  [A loss: 10.314039, acc: 0.000000]\n",
      "712: [D loss: 0.911367, acc: 0.603516]  [A loss: 0.469625, acc: 0.765625]\n",
      "713: [D loss: 2.125816, acc: 0.507812]  [A loss: 11.094021, acc: 0.000000]\n",
      "714: [D loss: 1.212066, acc: 0.582031]  [A loss: 0.007683, acc: 1.000000]\n",
      "715: [D loss: 4.106013, acc: 0.500000]  [A loss: 4.350306, acc: 0.000000]\n",
      "716: [D loss: 1.445452, acc: 0.546875]  [A loss: 7.968525, acc: 0.000000]\n",
      "717: [D loss: 0.784516, acc: 0.623047]  [A loss: 1.076064, acc: 0.445312]\n",
      "718: [D loss: 2.572438, acc: 0.498047]  [A loss: 11.734377, acc: 0.000000]\n",
      "719: [D loss: 1.656662, acc: 0.539062]  [A loss: 0.002146, acc: 1.000000]\n",
      "720: [D loss: 5.476267, acc: 0.500000]  [A loss: 2.385832, acc: 0.113281]\n",
      "721: [D loss: 2.447358, acc: 0.505859]  [A loss: 9.736769, acc: 0.000000]\n",
      "722: [D loss: 0.882842, acc: 0.582031]  [A loss: 0.154712, acc: 0.953125]\n",
      "723: [D loss: 2.421010, acc: 0.509766]  [A loss: 9.842125, acc: 0.000000]\n",
      "724: [D loss: 1.003413, acc: 0.617188]  [A loss: 0.055601, acc: 0.996094]\n",
      "725: [D loss: 3.069818, acc: 0.507812]  [A loss: 8.124279, acc: 0.000000]\n",
      "726: [D loss: 0.908098, acc: 0.587891]  [A loss: 1.772884, acc: 0.191406]\n",
      "727: [D loss: 2.273266, acc: 0.503906]  [A loss: 10.262595, acc: 0.000000]\n",
      "728: [D loss: 1.062324, acc: 0.572266]  [A loss: 0.054229, acc: 0.980469]\n",
      "729: [D loss: 3.095526, acc: 0.501953]  [A loss: 8.882460, acc: 0.000000]\n",
      "730: [D loss: 0.893116, acc: 0.605469]  [A loss: 1.239929, acc: 0.367188]\n",
      "731: [D loss: 2.444845, acc: 0.503906]  [A loss: 11.831450, acc: 0.000000]\n",
      "732: [D loss: 1.820536, acc: 0.523438]  [A loss: 0.002479, acc: 1.000000]\n",
      "733: [D loss: 5.554318, acc: 0.500000]  [A loss: 1.781369, acc: 0.253906]\n",
      "734: [D loss: 2.544708, acc: 0.509766]  [A loss: 10.684122, acc: 0.000000]\n",
      "735: [D loss: 1.105234, acc: 0.568359]  [A loss: 0.056732, acc: 0.984375]\n",
      "736: [D loss: 3.222237, acc: 0.498047]  [A loss: 7.732610, acc: 0.000000]\n",
      "737: [D loss: 0.914358, acc: 0.589844]  [A loss: 3.050583, acc: 0.027344]\n",
      "738: [D loss: 2.146102, acc: 0.515625]  [A loss: 9.845874, acc: 0.000000]\n",
      "739: [D loss: 1.003327, acc: 0.605469]  [A loss: 0.072366, acc: 0.984375]\n",
      "740: [D loss: 2.925937, acc: 0.500000]  [A loss: 9.844206, acc: 0.000000]\n",
      "741: [D loss: 0.922654, acc: 0.611328]  [A loss: 0.145603, acc: 0.941406]\n",
      "742: [D loss: 2.140081, acc: 0.513672]  [A loss: 9.680044, acc: 0.000000]\n",
      "743: [D loss: 0.849858, acc: 0.638672]  [A loss: 0.196918, acc: 0.902344]\n",
      "744: [D loss: 2.339535, acc: 0.515625]  [A loss: 9.968937, acc: 0.000000]\n",
      "745: [D loss: 1.073050, acc: 0.570312]  [A loss: 0.031846, acc: 0.996094]\n",
      "746: [D loss: 3.078191, acc: 0.501953]  [A loss: 8.182739, acc: 0.000000]\n",
      "747: [D loss: 0.874840, acc: 0.593750]  [A loss: 1.863497, acc: 0.203125]\n",
      "748: [D loss: 2.450150, acc: 0.505859]  [A loss: 11.583085, acc: 0.000000]\n",
      "749: [D loss: 1.660709, acc: 0.535156]  [A loss: 0.001616, acc: 1.000000]\n",
      "750: [D loss: 5.380951, acc: 0.500000]  [A loss: 2.835672, acc: 0.082031]\n",
      "751: [D loss: 2.330132, acc: 0.501953]  [A loss: 9.915150, acc: 0.000000]\n",
      "752: [D loss: 0.992080, acc: 0.597656]  [A loss: 0.066511, acc: 0.988281]\n",
      "753: [D loss: 2.867901, acc: 0.498047]  [A loss: 8.719202, acc: 0.000000]\n",
      "754: [D loss: 0.889875, acc: 0.585938]  [A loss: 0.427331, acc: 0.753906]\n",
      "755: [D loss: 2.084291, acc: 0.511719]  [A loss: 9.623492, acc: 0.000000]\n",
      "756: [D loss: 1.063865, acc: 0.572266]  [A loss: 0.096055, acc: 0.968750]\n",
      "757: [D loss: 2.984617, acc: 0.505859]  [A loss: 9.314467, acc: 0.000000]\n",
      "758: [D loss: 0.929581, acc: 0.582031]  [A loss: 0.292889, acc: 0.867188]\n",
      "759: [D loss: 2.316329, acc: 0.517578]  [A loss: 10.042133, acc: 0.000000]\n",
      "760: [D loss: 1.007244, acc: 0.591797]  [A loss: 0.017891, acc: 1.000000]\n",
      "761: [D loss: 3.358391, acc: 0.500000]  [A loss: 6.991357, acc: 0.000000]\n",
      "762: [D loss: 0.876262, acc: 0.619141]  [A loss: 3.544669, acc: 0.015625]\n",
      "763: [D loss: 2.097795, acc: 0.519531]  [A loss: 11.327740, acc: 0.000000]\n",
      "764: [D loss: 1.744130, acc: 0.539062]  [A loss: 0.002526, acc: 1.000000]\n",
      "765: [D loss: 5.306316, acc: 0.500000]  [A loss: 3.429948, acc: 0.050781]\n",
      "766: [D loss: 2.695211, acc: 0.501953]  [A loss: 11.010221, acc: 0.000000]\n",
      "767: [D loss: 0.955685, acc: 0.580078]  [A loss: 0.158740, acc: 0.957031]\n",
      "768: [D loss: 2.507381, acc: 0.511719]  [A loss: 10.031046, acc: 0.000000]\n",
      "769: [D loss: 1.038910, acc: 0.587891]  [A loss: 0.077562, acc: 0.984375]\n",
      "770: [D loss: 2.739700, acc: 0.505859]  [A loss: 8.655100, acc: 0.000000]\n",
      "771: [D loss: 0.748863, acc: 0.681641]  [A loss: 0.445916, acc: 0.773438]\n",
      "772: [D loss: 1.948363, acc: 0.513672]  [A loss: 9.833731, acc: 0.000000]\n",
      "773: [D loss: 0.865133, acc: 0.652344]  [A loss: 0.104841, acc: 0.964844]\n",
      "774: [D loss: 2.399669, acc: 0.507812]  [A loss: 9.525740, acc: 0.000000]\n",
      "775: [D loss: 0.935112, acc: 0.619141]  [A loss: 0.228979, acc: 0.906250]\n",
      "776: [D loss: 2.085434, acc: 0.509766]  [A loss: 11.170233, acc: 0.000000]\n",
      "777: [D loss: 1.262558, acc: 0.582031]  [A loss: 0.011417, acc: 1.000000]\n",
      "778: [D loss: 4.235112, acc: 0.500000]  [A loss: 5.447524, acc: 0.003906]\n",
      "779: [D loss: 1.427189, acc: 0.552734]  [A loss: 9.509203, acc: 0.000000]\n",
      "780: [D loss: 0.941415, acc: 0.601562]  [A loss: 0.136187, acc: 0.960938]\n",
      "781: [D loss: 2.370473, acc: 0.523438]  [A loss: 10.576017, acc: 0.000000]\n",
      "782: [D loss: 1.124292, acc: 0.593750]  [A loss: 0.009896, acc: 1.000000]\n",
      "783: [D loss: 3.906111, acc: 0.500000]  [A loss: 5.774079, acc: 0.003906]\n",
      "784: [D loss: 1.275738, acc: 0.593750]  [A loss: 7.701998, acc: 0.000000]\n",
      "785: [D loss: 0.904191, acc: 0.582031]  [A loss: 3.495349, acc: 0.046875]\n",
      "786: [D loss: 2.156909, acc: 0.505859]  [A loss: 13.001883, acc: 0.000000]\n",
      "787: [D loss: 2.620060, acc: 0.513672]  [A loss: 0.000338, acc: 1.000000]\n",
      "788: [D loss: 6.525271, acc: 0.500000]  [A loss: 0.210916, acc: 0.917969]\n",
      "789: [D loss: 2.471873, acc: 0.515625]  [A loss: 11.852959, acc: 0.000000]\n",
      "790: [D loss: 0.892275, acc: 0.636719]  [A loss: 0.225446, acc: 0.890625]\n",
      "791: [D loss: 2.470176, acc: 0.517578]  [A loss: 11.298304, acc: 0.000000]\n",
      "792: [D loss: 1.395455, acc: 0.539062]  [A loss: 0.012967, acc: 1.000000]\n",
      "793: [D loss: 4.438680, acc: 0.501953]  [A loss: 4.464821, acc: 0.007812]\n",
      "794: [D loss: 1.745005, acc: 0.537109]  [A loss: 9.592854, acc: 0.000000]\n",
      "795: [D loss: 0.844547, acc: 0.626953]  [A loss: 0.359859, acc: 0.828125]\n",
      "796: [D loss: 2.486269, acc: 0.509766]  [A loss: 11.391821, acc: 0.000000]\n",
      "797: [D loss: 1.415182, acc: 0.541016]  [A loss: 0.005977, acc: 1.000000]\n",
      "798: [D loss: 4.869610, acc: 0.500000]  [A loss: 3.968143, acc: 0.015625]\n",
      "799: [D loss: 2.211257, acc: 0.519531]  [A loss: 10.694568, acc: 0.000000]\n",
      "800: [D loss: 1.145659, acc: 0.572266]  [A loss: 0.045647, acc: 0.984375]\n",
      "801: [D loss: 3.517631, acc: 0.500000]  [A loss: 9.300550, acc: 0.000000]\n",
      "802: [D loss: 1.008291, acc: 0.595703]  [A loss: 2.084162, acc: 0.195312]\n",
      "803: [D loss: 2.621489, acc: 0.515625]  [A loss: 12.222991, acc: 0.000000]\n",
      "804: [D loss: 1.759310, acc: 0.521484]  [A loss: 0.003486, acc: 1.000000]\n",
      "805: [D loss: 5.938187, acc: 0.500000]  [A loss: 2.093124, acc: 0.210938]\n",
      "806: [D loss: 2.664649, acc: 0.513672]  [A loss: 10.736038, acc: 0.000000]\n",
      "807: [D loss: 1.017879, acc: 0.578125]  [A loss: 0.174622, acc: 0.914062]\n",
      "808: [D loss: 2.370607, acc: 0.503906]  [A loss: 11.416941, acc: 0.000000]\n",
      "809: [D loss: 1.233782, acc: 0.562500]  [A loss: 0.014446, acc: 1.000000]\n",
      "810: [D loss: 4.358535, acc: 0.501953]  [A loss: 6.193480, acc: 0.000000]\n",
      "811: [D loss: 1.528300, acc: 0.558594]  [A loss: 9.341995, acc: 0.000000]\n",
      "812: [D loss: 0.948427, acc: 0.605469]  [A loss: 0.860724, acc: 0.542969]\n",
      "813: [D loss: 2.463286, acc: 0.501953]  [A loss: 13.057571, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814: [D loss: 2.326491, acc: 0.513672]  [A loss: 0.001070, acc: 1.000000]\n",
      "815: [D loss: 6.062399, acc: 0.500000]  [A loss: 0.924136, acc: 0.546875]\n",
      "816: [D loss: 2.557844, acc: 0.507812]  [A loss: 10.790518, acc: 0.000000]\n",
      "817: [D loss: 1.023844, acc: 0.544922]  [A loss: 1.203753, acc: 0.460938]\n",
      "818: [D loss: 2.707287, acc: 0.505859]  [A loss: 12.423460, acc: 0.000000]\n",
      "819: [D loss: 1.882139, acc: 0.519531]  [A loss: 0.002670, acc: 1.000000]\n",
      "820: [D loss: 5.492286, acc: 0.500000]  [A loss: 2.720038, acc: 0.160156]\n",
      "821: [D loss: 2.970702, acc: 0.503906]  [A loss: 11.238516, acc: 0.000000]\n",
      "822: [D loss: 1.074692, acc: 0.544922]  [A loss: 0.436639, acc: 0.761719]\n",
      "823: [D loss: 2.624240, acc: 0.500000]  [A loss: 11.987561, acc: 0.000000]\n",
      "824: [D loss: 1.418047, acc: 0.527344]  [A loss: 0.013313, acc: 0.996094]\n",
      "825: [D loss: 4.094807, acc: 0.500000]  [A loss: 7.888514, acc: 0.000000]\n",
      "826: [D loss: 1.297011, acc: 0.556641]  [A loss: 7.559012, acc: 0.000000]\n",
      "827: [D loss: 1.139726, acc: 0.593750]  [A loss: 6.759745, acc: 0.000000]\n",
      "828: [D loss: 1.333337, acc: 0.554688]  [A loss: 8.403768, acc: 0.000000]\n",
      "829: [D loss: 0.959841, acc: 0.619141]  [A loss: 3.166662, acc: 0.066406]\n",
      "830: [D loss: 2.408337, acc: 0.505859]  [A loss: 13.770699, acc: 0.000000]\n",
      "831: [D loss: 4.602715, acc: 0.500000]  [A loss: 0.003676, acc: 1.000000]\n",
      "832: [D loss: 5.831436, acc: 0.500000]  [A loss: 1.072766, acc: 0.542969]\n",
      "833: [D loss: 2.419643, acc: 0.523438]  [A loss: 11.911944, acc: 0.000000]\n",
      "834: [D loss: 1.310391, acc: 0.548828]  [A loss: 0.024421, acc: 1.000000]\n",
      "835: [D loss: 3.386251, acc: 0.500000]  [A loss: 8.442068, acc: 0.000000]\n",
      "836: [D loss: 1.141250, acc: 0.599609]  [A loss: 7.044820, acc: 0.000000]\n",
      "837: [D loss: 1.248646, acc: 0.556641]  [A loss: 8.541893, acc: 0.000000]\n",
      "838: [D loss: 0.962616, acc: 0.568359]  [A loss: 1.098142, acc: 0.472656]\n",
      "839: [D loss: 2.350352, acc: 0.521484]  [A loss: 13.034594, acc: 0.000000]\n",
      "840: [D loss: 2.754208, acc: 0.501953]  [A loss: 0.005432, acc: 1.000000]\n",
      "841: [D loss: 6.084116, acc: 0.500000]  [A loss: 0.471311, acc: 0.761719]\n",
      "842: [D loss: 2.730922, acc: 0.519531]  [A loss: 12.050417, acc: 0.000000]\n",
      "843: [D loss: 1.084278, acc: 0.582031]  [A loss: 0.074263, acc: 0.980469]\n",
      "844: [D loss: 3.030415, acc: 0.501953]  [A loss: 9.062450, acc: 0.000000]\n",
      "845: [D loss: 0.884022, acc: 0.601562]  [A loss: 1.206095, acc: 0.421875]\n",
      "846: [D loss: 2.436324, acc: 0.527344]  [A loss: 11.343577, acc: 0.000000]\n",
      "847: [D loss: 1.339010, acc: 0.517578]  [A loss: 0.007687, acc: 1.000000]\n",
      "848: [D loss: 4.740499, acc: 0.500000]  [A loss: 4.858299, acc: 0.015625]\n",
      "849: [D loss: 1.933091, acc: 0.513672]  [A loss: 10.530390, acc: 0.000000]\n",
      "850: [D loss: 1.096694, acc: 0.556641]  [A loss: 0.095043, acc: 0.972656]\n",
      "851: [D loss: 2.835995, acc: 0.507812]  [A loss: 9.484118, acc: 0.000000]\n",
      "852: [D loss: 0.954779, acc: 0.611328]  [A loss: 0.189343, acc: 0.917969]\n",
      "853: [D loss: 2.433977, acc: 0.505859]  [A loss: 10.065351, acc: 0.000000]\n",
      "854: [D loss: 0.859462, acc: 0.625000]  [A loss: 0.120279, acc: 0.957031]\n",
      "855: [D loss: 2.576799, acc: 0.507812]  [A loss: 10.067566, acc: 0.000000]\n",
      "856: [D loss: 1.035274, acc: 0.556641]  [A loss: 0.086238, acc: 0.976562]\n",
      "857: [D loss: 2.936331, acc: 0.505859]  [A loss: 9.578043, acc: 0.000000]\n",
      "858: [D loss: 0.921938, acc: 0.621094]  [A loss: 0.252227, acc: 0.882812]\n",
      "859: [D loss: 2.452533, acc: 0.509766]  [A loss: 11.149277, acc: 0.000000]\n",
      "860: [D loss: 1.149237, acc: 0.593750]  [A loss: 0.012918, acc: 0.996094]\n",
      "861: [D loss: 3.863647, acc: 0.500000]  [A loss: 6.474134, acc: 0.007812]\n",
      "862: [D loss: 1.207605, acc: 0.578125]  [A loss: 7.525236, acc: 0.000000]\n",
      "863: [D loss: 0.908330, acc: 0.605469]  [A loss: 3.258057, acc: 0.054688]\n",
      "864: [D loss: 2.362829, acc: 0.509766]  [A loss: 12.646495, acc: 0.000000]\n",
      "865: [D loss: 2.299359, acc: 0.511719]  [A loss: 0.000871, acc: 1.000000]\n",
      "866: [D loss: 6.336714, acc: 0.500000]  [A loss: 0.388167, acc: 0.789062]\n",
      "867: [D loss: 2.509011, acc: 0.503906]  [A loss: 12.066754, acc: 0.000000]\n",
      "868: [D loss: 1.025532, acc: 0.619141]  [A loss: 0.103675, acc: 0.972656]\n",
      "869: [D loss: 2.906678, acc: 0.505859]  [A loss: 10.276780, acc: 0.000000]\n",
      "870: [D loss: 0.943133, acc: 0.607422]  [A loss: 0.594911, acc: 0.710938]\n",
      "871: [D loss: 2.283156, acc: 0.519531]  [A loss: 11.463629, acc: 0.000000]\n",
      "872: [D loss: 1.221848, acc: 0.568359]  [A loss: 0.007041, acc: 1.000000]\n",
      "873: [D loss: 4.501052, acc: 0.500000]  [A loss: 6.768764, acc: 0.000000]\n",
      "874: [D loss: 1.353023, acc: 0.558594]  [A loss: 9.160984, acc: 0.000000]\n",
      "875: [D loss: 0.919321, acc: 0.605469]  [A loss: 1.508050, acc: 0.367188]\n",
      "876: [D loss: 2.720201, acc: 0.501953]  [A loss: 13.074598, acc: 0.000000]\n",
      "877: [D loss: 2.687314, acc: 0.519531]  [A loss: 0.000474, acc: 1.000000]\n",
      "878: [D loss: 6.519207, acc: 0.500000]  [A loss: 0.126596, acc: 0.949219]\n",
      "879: [D loss: 2.851781, acc: 0.507812]  [A loss: 9.414517, acc: 0.000000]\n",
      "880: [D loss: 1.110759, acc: 0.605469]  [A loss: 5.565489, acc: 0.000000]\n",
      "881: [D loss: 2.047302, acc: 0.507812]  [A loss: 12.708401, acc: 0.000000]\n",
      "882: [D loss: 1.790659, acc: 0.535156]  [A loss: 0.004253, acc: 1.000000]\n",
      "883: [D loss: 5.571453, acc: 0.500000]  [A loss: 1.037772, acc: 0.566406]\n",
      "884: [D loss: 2.589138, acc: 0.527344]  [A loss: 10.635679, acc: 0.000000]\n",
      "885: [D loss: 0.980299, acc: 0.585938]  [A loss: 0.819897, acc: 0.570312]\n",
      "886: [D loss: 2.298769, acc: 0.519531]  [A loss: 11.353621, acc: 0.000000]\n",
      "887: [D loss: 1.184189, acc: 0.548828]  [A loss: 0.139015, acc: 0.949219]\n",
      "888: [D loss: 2.890082, acc: 0.507812]  [A loss: 11.177189, acc: 0.000000]\n",
      "889: [D loss: 1.205971, acc: 0.570312]  [A loss: 0.119929, acc: 0.964844]\n",
      "890: [D loss: 3.164396, acc: 0.503906]  [A loss: 10.135248, acc: 0.000000]\n",
      "891: [D loss: 0.931553, acc: 0.597656]  [A loss: 0.489818, acc: 0.769531]\n",
      "892: [D loss: 2.220928, acc: 0.521484]  [A loss: 10.700980, acc: 0.000000]\n",
      "893: [D loss: 0.961614, acc: 0.628906]  [A loss: 0.066679, acc: 0.980469]\n",
      "894: [D loss: 3.030787, acc: 0.509766]  [A loss: 9.647568, acc: 0.000000]\n",
      "895: [D loss: 0.860913, acc: 0.626953]  [A loss: 1.109854, acc: 0.468750]\n",
      "896: [D loss: 2.501568, acc: 0.519531]  [A loss: 12.378770, acc: 0.000000]\n",
      "897: [D loss: 1.642091, acc: 0.513672]  [A loss: 0.001361, acc: 1.000000]\n",
      "898: [D loss: 5.669016, acc: 0.500000]  [A loss: 1.821416, acc: 0.378906]\n",
      "899: [D loss: 2.930547, acc: 0.505859]  [A loss: 12.593576, acc: 0.000000]\n",
      "900: [D loss: 1.273764, acc: 0.562500]  [A loss: 0.045695, acc: 0.988281]\n",
      "901: [D loss: 3.549486, acc: 0.505859]  [A loss: 11.042733, acc: 0.000000]\n",
      "902: [D loss: 0.996989, acc: 0.605469]  [A loss: 0.249689, acc: 0.875000]\n",
      "903: [D loss: 2.478107, acc: 0.515625]  [A loss: 11.636869, acc: 0.000000]\n",
      "904: [D loss: 1.131504, acc: 0.580078]  [A loss: 0.026454, acc: 0.996094]\n",
      "905: [D loss: 3.661329, acc: 0.500000]  [A loss: 10.112673, acc: 0.000000]\n",
      "906: [D loss: 0.961216, acc: 0.601562]  [A loss: 3.347038, acc: 0.074219]\n",
      "907: [D loss: 2.830786, acc: 0.517578]  [A loss: 12.947014, acc: 0.000000]\n",
      "908: [D loss: 2.100850, acc: 0.533203]  [A loss: 0.000184, acc: 1.000000]\n",
      "909: [D loss: 6.527834, acc: 0.500000]  [A loss: 0.542303, acc: 0.726562]\n",
      "910: [D loss: 2.924618, acc: 0.507812]  [A loss: 12.027730, acc: 0.000000]\n",
      "911: [D loss: 0.977244, acc: 0.623047]  [A loss: 0.131810, acc: 0.945312]\n",
      "912: [D loss: 2.990601, acc: 0.500000]  [A loss: 11.533562, acc: 0.000000]\n",
      "913: [D loss: 1.089672, acc: 0.587891]  [A loss: 0.077652, acc: 0.980469]\n",
      "914: [D loss: 3.118035, acc: 0.505859]  [A loss: 10.968248, acc: 0.000000]\n",
      "915: [D loss: 1.025991, acc: 0.595703]  [A loss: 0.241237, acc: 0.902344]\n",
      "916: [D loss: 2.414334, acc: 0.519531]  [A loss: 11.069338, acc: 0.000000]\n",
      "917: [D loss: 0.961852, acc: 0.611328]  [A loss: 0.052768, acc: 0.988281]\n",
      "918: [D loss: 3.025211, acc: 0.511719]  [A loss: 9.326069, acc: 0.000000]\n",
      "919: [D loss: 0.840371, acc: 0.656250]  [A loss: 2.723731, acc: 0.093750]\n",
      "920: [D loss: 2.913546, acc: 0.505859]  [A loss: 13.275590, acc: 0.000000]\n",
      "921: [D loss: 2.234171, acc: 0.529297]  [A loss: 0.000810, acc: 1.000000]\n",
      "922: [D loss: 6.760741, acc: 0.500000]  [A loss: 0.195516, acc: 0.914062]\n",
      "923: [D loss: 2.774687, acc: 0.505859]  [A loss: 11.129530, acc: 0.000000]\n",
      "924: [D loss: 1.044945, acc: 0.603516]  [A loss: 2.494650, acc: 0.191406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "925: [D loss: 2.969105, acc: 0.509766]  [A loss: 11.953415, acc: 0.000000]\n",
      "926: [D loss: 1.062994, acc: 0.595703]  [A loss: 0.022952, acc: 0.996094]\n",
      "927: [D loss: 3.728868, acc: 0.501953]  [A loss: 8.910909, acc: 0.000000]\n",
      "928: [D loss: 1.111070, acc: 0.589844]  [A loss: 4.896560, acc: 0.011719]\n",
      "929: [D loss: 2.490287, acc: 0.517578]  [A loss: 13.624281, acc: 0.000000]\n",
      "930: [D loss: 2.972555, acc: 0.498047]  [A loss: 0.000590, acc: 1.000000]\n",
      "931: [D loss: 6.271840, acc: 0.500000]  [A loss: 0.528188, acc: 0.726562]\n",
      "932: [D loss: 2.918244, acc: 0.509766]  [A loss: 11.447428, acc: 0.000000]\n",
      "933: [D loss: 1.156206, acc: 0.550781]  [A loss: 0.672046, acc: 0.652344]\n",
      "934: [D loss: 2.631336, acc: 0.509766]  [A loss: 12.324687, acc: 0.000000]\n",
      "935: [D loss: 1.395727, acc: 0.562500]  [A loss: 0.016604, acc: 0.996094]\n",
      "936: [D loss: 4.143648, acc: 0.500000]  [A loss: 7.410161, acc: 0.000000]\n",
      "937: [D loss: 1.808174, acc: 0.546875]  [A loss: 10.880534, acc: 0.000000]\n",
      "938: [D loss: 1.066182, acc: 0.566406]  [A loss: 0.138956, acc: 0.960938]\n",
      "939: [D loss: 2.754779, acc: 0.509766]  [A loss: 11.970160, acc: 0.000000]\n",
      "940: [D loss: 1.306513, acc: 0.562500]  [A loss: 0.010183, acc: 1.000000]\n",
      "941: [D loss: 4.347400, acc: 0.500000]  [A loss: 6.652433, acc: 0.000000]\n",
      "942: [D loss: 1.878900, acc: 0.542969]  [A loss: 12.022820, acc: 0.000000]\n",
      "943: [D loss: 1.521242, acc: 0.529297]  [A loss: 0.005806, acc: 1.000000]\n",
      "944: [D loss: 4.774117, acc: 0.500000]  [A loss: 6.614067, acc: 0.007812]\n",
      "945: [D loss: 1.887924, acc: 0.533203]  [A loss: 11.927404, acc: 0.000000]\n",
      "946: [D loss: 1.595319, acc: 0.554688]  [A loss: 0.010308, acc: 0.996094]\n",
      "947: [D loss: 4.944205, acc: 0.500000]  [A loss: 4.410252, acc: 0.007812]\n",
      "948: [D loss: 2.581049, acc: 0.527344]  [A loss: 11.809752, acc: 0.000000]\n",
      "949: [D loss: 1.392033, acc: 0.539062]  [A loss: 0.007598, acc: 1.000000]\n",
      "950: [D loss: 4.587320, acc: 0.500000]  [A loss: 6.729854, acc: 0.003906]\n",
      "951: [D loss: 2.040103, acc: 0.515625]  [A loss: 12.208588, acc: 0.000000]\n",
      "952: [D loss: 1.317250, acc: 0.539062]  [A loss: 0.013094, acc: 0.996094]\n",
      "953: [D loss: 4.956452, acc: 0.501953]  [A loss: 5.652530, acc: 0.019531]\n",
      "954: [D loss: 2.559916, acc: 0.521484]  [A loss: 13.142944, acc: 0.000000]\n",
      "955: [D loss: 2.891363, acc: 0.501953]  [A loss: 0.001942, acc: 1.000000]\n",
      "956: [D loss: 6.150753, acc: 0.500000]  [A loss: 0.709047, acc: 0.679688]\n",
      "957: [D loss: 2.760077, acc: 0.507812]  [A loss: 12.098677, acc: 0.000000]\n",
      "958: [D loss: 1.235284, acc: 0.558594]  [A loss: 0.288479, acc: 0.851562]\n",
      "959: [D loss: 2.606501, acc: 0.509766]  [A loss: 12.095763, acc: 0.000000]\n",
      "960: [D loss: 1.262396, acc: 0.570312]  [A loss: 0.049260, acc: 0.996094]\n",
      "961: [D loss: 3.502290, acc: 0.507812]  [A loss: 10.402962, acc: 0.000000]\n",
      "962: [D loss: 1.029098, acc: 0.619141]  [A loss: 2.430353, acc: 0.199219]\n",
      "963: [D loss: 2.992054, acc: 0.513672]  [A loss: 12.060822, acc: 0.000000]\n",
      "964: [D loss: 1.451948, acc: 0.539062]  [A loss: 0.023077, acc: 0.992188]\n",
      "965: [D loss: 4.843658, acc: 0.500000]  [A loss: 7.186012, acc: 0.000000]\n",
      "966: [D loss: 1.936974, acc: 0.539062]  [A loss: 10.856754, acc: 0.000000]\n",
      "967: [D loss: 0.999564, acc: 0.576172]  [A loss: 0.629385, acc: 0.675781]\n",
      "968: [D loss: 2.804147, acc: 0.513672]  [A loss: 13.641403, acc: 0.000000]\n",
      "969: [D loss: 4.002019, acc: 0.500000]  [A loss: 0.008337, acc: 1.000000]\n",
      "970: [D loss: 5.772355, acc: 0.500000]  [A loss: 0.906778, acc: 0.597656]\n",
      "971: [D loss: 2.622192, acc: 0.548828]  [A loss: 11.695066, acc: 0.000000]\n",
      "972: [D loss: 1.193418, acc: 0.564453]  [A loss: 2.766684, acc: 0.105469]\n",
      "973: [D loss: 3.594748, acc: 0.507812]  [A loss: 12.627939, acc: 0.000000]\n",
      "974: [D loss: 2.077178, acc: 0.517578]  [A loss: 0.005676, acc: 1.000000]\n",
      "975: [D loss: 5.477900, acc: 0.500000]  [A loss: 1.597458, acc: 0.351562]\n",
      "976: [D loss: 3.149702, acc: 0.507812]  [A loss: 12.010578, acc: 0.000000]\n",
      "977: [D loss: 1.217389, acc: 0.537109]  [A loss: 0.081750, acc: 0.976562]\n",
      "978: [D loss: 3.529622, acc: 0.511719]  [A loss: 9.698339, acc: 0.000000]\n",
      "979: [D loss: 1.427224, acc: 0.548828]  [A loss: 2.931935, acc: 0.144531]\n",
      "980: [D loss: 2.714031, acc: 0.523438]  [A loss: 11.067936, acc: 0.000000]\n",
      "981: [D loss: 1.232966, acc: 0.525391]  [A loss: 0.107961, acc: 0.964844]\n",
      "982: [D loss: 2.938394, acc: 0.498047]  [A loss: 11.889219, acc: 0.000000]\n",
      "983: [D loss: 1.586455, acc: 0.539062]  [A loss: 0.016749, acc: 0.996094]\n",
      "984: [D loss: 5.016274, acc: 0.501953]  [A loss: 2.268663, acc: 0.199219]\n",
      "985: [D loss: 3.105499, acc: 0.513672]  [A loss: 11.265558, acc: 0.000000]\n",
      "986: [D loss: 0.979154, acc: 0.585938]  [A loss: 0.266116, acc: 0.894531]\n",
      "987: [D loss: 2.547645, acc: 0.525391]  [A loss: 11.305375, acc: 0.000000]\n",
      "988: [D loss: 1.361196, acc: 0.542969]  [A loss: 0.025796, acc: 0.992188]\n",
      "989: [D loss: 3.810167, acc: 0.501953]  [A loss: 8.893263, acc: 0.000000]\n",
      "990: [D loss: 1.201409, acc: 0.589844]  [A loss: 7.533794, acc: 0.000000]\n",
      "991: [D loss: 1.598830, acc: 0.541016]  [A loss: 10.879768, acc: 0.000000]\n",
      "992: [D loss: 1.307702, acc: 0.574219]  [A loss: 0.051240, acc: 0.984375]\n",
      "993: [D loss: 3.851775, acc: 0.501953]  [A loss: 8.808658, acc: 0.000000]\n",
      "994: [D loss: 1.197335, acc: 0.599609]  [A loss: 5.715699, acc: 0.000000]\n",
      "995: [D loss: 2.137846, acc: 0.539062]  [A loss: 13.194012, acc: 0.000000]\n",
      "996: [D loss: 3.543646, acc: 0.494141]  [A loss: 0.005377, acc: 1.000000]\n",
      "997: [D loss: 5.847166, acc: 0.500000]  [A loss: 0.432919, acc: 0.792969]\n",
      "998: [D loss: 2.968205, acc: 0.541016]  [A loss: 13.583043, acc: 0.000000]\n",
      "999: [D loss: 2.495081, acc: 0.517578]  [A loss: 0.001229, acc: 1.000000]\n"
     ]
    }
   ],
   "source": [
    "# MNIST DCGAN 초기화 및 훈련\n",
    "mnist_dcgan = MNIST_DCGAN(X_train_keras)\n",
    "timer = ElapsedTimer()\n",
    "mnist_dcgan.train(train_steps=1000, batch_size=256, save_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 4.34897359960609 hr \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 합성 MNIST 이미지 배치 생성\n",
    "timer.elapsed_time()\n",
    "mnist_dcgan.plot_images(fake=True)\n",
    "mnist_dcgan.plot_images(fake=False, save2file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
